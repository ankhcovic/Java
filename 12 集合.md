# 集合

## [1.1. 集合概述](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_11-集合概述)

### [1.1.1. Java 集合概览](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_111-java-集合概览)

从下图可以看出，在 Java 中除了以 `Map` 结尾的类之外， 其他类都实现了 `Collection` 接口。

并且，以 `Map` 结尾的类都实现了 `Map` 接口。

<img src="C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\java-collection-hierarchy.png" alt="img" style="zoom: 80%;" />



![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\2243690-9cd9c896e0d512ed.gif)

### [1.1.2. 说说 List,Set,Map 三者的区别？](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_112-说说-listsetmap-三者的区别？)

- `List`(对付顺序的好帮手)： 存储的元素是**有序的、可重复**的。
- `Set`(注重独一无二的性质): 存储的元素是**无序的、不可重复**的。
- `Map`(用 Key 来搜索的专家): 使用**键值对（kye-value）存储**，类似于数学上的函数 y=f(x)，“x”代表 key，"y"代表 value，**Key 是无序的、不可重复的，value 是无序的、可重复的**，每个键最多映射到一个值。

### [1.1.3. 集合框架底层数据结构总结](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_113-集合框架底层数据结构总结)

先来看一下 `Collection` 接口下面的集合。

#### [1.1.3.1. List](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_1131-list)

- `Arraylist`： `Object[]`数组
- `Vector`：`Object[]`数组
- `LinkedList`： 双向链表(JDK1.6 之前为循环链表，JDK1.7 取消了循环)

#### [1.1.3.2. Set](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_1132-set)

- `HashSet`（无序，唯一）: 基于 `HashMap` 实现的，底层采用 `HashMap` 来保存元素
- `LinkedHashSet`：`LinkedHashSet` 是 `HashSet` 的子类，并且其内部是通过 `LinkedHashMap` 来实现的。有点类似于我们之前说的 `LinkedHashMap` 其内部是基于 `HashMap` 实现一样，不过还是有一点点区别的
- `TreeSet`（有序，唯一）： 红黑树(自平衡的排序二叉树)

再来看看 `Map` 接口下面的集合。

#### [1.1.3.3. Map](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_1133-map)

- `HashMap`： JDK1.8 之前 `HashMap` 由**数组+链表**组成的，数组是 `HashMap` 的主体，链表则是主要为了**解决哈希冲突**而存在的（“拉链法”解决冲突）。**JDK1.8** 以后在解决哈希冲突时有了较大的变化，当**链表长度大于阈值（默认为 8）**（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择**先进行数组扩容**，而不是转换为红黑树）时，**将链表转化为红黑树，以减少搜索时间**
- `LinkedHashMap`： `LinkedHashMap` 继承自 `HashMap`，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，`LinkedHashMap` 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。详细可以查看：[《LinkedHashMap 源码详细分析（JDK1.8）》](https://www.imooc.com/article/22931)
- `Hashtable`： 数组+链表组成的，数组是 `HashMap` 的主体，链表则是主要为了解决哈希冲突而存在的
- `TreeMap`： 红黑树（自平衡的排序二叉树）

### [1.1.4. 如何选用集合?](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_114-如何选用集合)

主要根据集合的特点来选用

- 需要根据键值获取到元素值时就选用 `Map` 接口下的集合

  - 需要排序时选择 `TreeMap`
  - 不需要排序时就选择 `HashMap`
  - 需要保证线程安全就选用 `ConcurrentHashMap`。

- 当我们只需要存放元素值时，就选择实现`Collection` 接口的集合

  - 需要保证元素唯一时选择实现 `Set` 接口的集合比如 `TreeSet` 或 `HashSet`
  - 不需要就选择实现 `List` 接口的比如 `ArrayList` 或 `LinkedList`

  然后再根据实现这些接口的集合的特点来选用。

### [1.1.5. 为什么要使用集合？](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_115-为什么要使用集合？)

当我们需要**保存一组类型相同**的数据的时候，我们应该是用一个容器来保存，这个容器就是数组，但是，使用数组存储对象具有一定的弊端， 因为我们**在实际开发中，存储的数据的类型是多种多样的**，于是，就出现了“集合”，集合同样也是用来存储多个数据的。

**数组**的缺点是一旦声明之后，**长度就不可变**了；同时，声明数组时的数据类型也决定了该数组存储的数据的类型；而且，数组存储的数据是**有序的、可重复的，特点单一**。 但是集合提高了数据存储的灵活性，Java 集合不仅可以用来**存储不同类型不同数量**的对象，还可以保存**具有映射关系**的数据。

## [1.2. Collection 子接口之 List](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_12-collection-子接口之-list)

### [1.2.1. Arraylist 和 Vector 的区别?](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_121-arraylist-和-vector-的区别)

- `ArrayList` 是 `List` 的主要实现类，底层使用 `Object[]`存储，适用于频繁的查找工作，线程不安全 ；
- `Vector` 是 `List` 的古老实现类，底层使用` Object[]` 存储，线程安全的。

### [1.2.2. Arraylist 与 LinkedList 区别?](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_122-arraylist-与-linkedlist-区别)

1. **是否保证线程安全：** `ArrayList` 和 `LinkedList` 都是**不同步的**，也就是**不保证线程安全**；

2. **底层数据结构：** `Arraylist` 底层使用的是 **`Object` 数组**；`LinkedList` 底层使用的是 **双向链表** 数据结构（JDK1.6 之前为循环链表，JDK1.7 取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！）

3. **插入和删除是否受元素位置的影响：** 

   ① **`ArrayList` 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。** 比如：执行`add(E e)`方法的时候， `ArrayList` 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是 O(1)。但是如果要在指定位置 i 插入和删除元素的话（`add(int index, E element)`）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 

   ② **`LinkedList` 采用链表存储，所以对于`add(E e)`方法的插入，删除元素时间复杂度不受元素位置的影响，近似 O(1)，如果是要在指定位置`i`插入和删除元素的话（`(add(int index, E element)`） 时间复杂度近似为`o(n))`因为需要先移动到指定位置再插入。**

4. **是否支持快速随机访问：** `LinkedList` 不支持高效的随机元素访问，而 `ArrayList` 支持。**快速随机访问RandomAccess**就是通过元素的序号快速获取元素对象(对应于`get(int index)`方法)。

5. **内存空间占用：** ArrayList 的空间浪费主要体现在在 list 列表的**结尾会预留一定的容量空间**，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗比 ArrayList 更多的空间（因为要存放直接后继succ和直接前驱pre以及数据）。

   > 扩容是1.5倍扩容，然后比较和所需内存大小，所以这个空间不一定是用完的

#### [1.2.2.1. 补充内容:双向链表和双向循环链表](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_1221-补充内容双向链表和双向循环链表)

**双向链表：** 包含两个指针，一个 prev 指向前一个节点，一个 next 指向后一个节点。

![双向链表](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\双向链表.png)

**双向循环链表：** 最后一个节点的 next 指向 head，而 head 的 prev 指向最后一个节点，构成一个环。

![双向循环链表](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\双向循环链表.png)

#### [1.2.2.2. 补充内容:RandomAccess 接口](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_1222-补充内容randomaccess-接口)

```
public interface RandomAccess {
}
```

`RandomAccess` 接口不过是一个标识，标识实现这个接口的类具有随机访问功能。

`ArrayList` 实现了 `RandomAccess` 接口， 而 `LinkedList` 没有实现。

`ArrayList` 底层是数组，而 `LinkedList` 底层是链表。数组天然支持随机访问，时间复杂度为 O(1)，所以称为快速随机访问。链表需要遍历到特定位置才能访问特定位置的元素，时间复杂度为 O(n)，所以不支持快速随机访问。

`ArrayList` 实现了 `RandomAccess` 接口，就表明了他具有快速随机访问功能。 `RandomAccess` 接口只是标识，并不是说 `ArrayList` 实现 `RandomAccess` 接口才具有快速随机访问功能的！

### [1.2.3. ArrayList 的扩容机制](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_123-说一说-arraylist-的扩容机制吧)

####[1.2.3.1. ArrayList 简介](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/ArrayList源码+扩容机制分析?id=_1-arraylist-简介)

`ArrayList` 的底层是数组队列，相当于动态数组。与 Java 中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用`ensureCapacity`操作来增加 `ArrayList` 实例的容量。这可以减少递增式再分配的数量。

`ArrayList`继承于 **`AbstractList`** ，实现了 **`List`**, **`RandomAccess`**, **`Cloneable`**, **`java.io.Serializable`** 这些接口。

```
public class ArrayList<E> extends AbstractList<E>
        implements List<E>, RandomAccess, Cloneable, java.io.Serializable{

  }
```

- `RandomAccess` 是一个标志接口，表明实现这个这个接口的 List 集合是支持**快速随机访问**的。在 `ArrayList` 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。
- `ArrayList` 实现了 **`Cloneable` 接口** ，即覆盖了函数`clone()`，能被克隆。可以实现浅拷贝
  - 浅拷贝只复制指向某个对象的指针，而不复制对象本身，新旧对象还是共享同一块内存， 所以**如果其中一个对象改变了这个地址，就会影响到另一个对象**。。
  - 浅拷贝对应的就是深拷贝，深拷贝是将一个对象从内存中完整的拷贝一份出来,从堆内存中开辟一个新的区域存放新对象,且**修改新对象不会影响原对象**。
- `ArrayList` 实现了 `java.io.Serializable `接口，这意味着`ArrayList`支持序列化，能通过序列化去传输。

####[1.2.3.2. ArrayList 扩容机制分析](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/ArrayList源码+扩容机制分析?id=_3-arraylist-扩容机制分析)

- **构造函数**

  - 无参构造

    - 默认构造函数，使用初始容量10构造一个空列表(无参数构造)

    **以无参数构造方法创建 `ArrayList` 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。**

    > `JDK7` new无参构造的`ArrayList`对象时，直接创建了长度是10的Object[]数组`elementData` 。`jdk7`中的`ArrayList`的对象的创建**类似于单例的饿汉式**，而`jdk8`中的`ArrayList`的对象的创建**类似于单例的懒汉式**。

  - 带初始长度

    - 初始容量大于0，创建initialCapacity大小的数组
    - 初始容量等于0，创建空数组
    - 初始容量小于0，抛出异常"Illegal Capacity: "

  - 包含指定集合元素的列表

    - 构造包含指定collection元素的列表，这些元素利用该集合的迭代器按顺序返回
    - 如果指定的集合为null，throws NullPointerException

-  **`ArrayList` 扩容机制**

  这里以无参构造函数创建的 ArrayList 为例分析

  - 以无参构造为例，首先会创建一个空的elementdata object[]类型

  - 当添加一个元素时，调用ensurecapacityinternal()，得到最小需要容量，最小需要容量为默认容量和size+1的最大值，如果说原来为空，那么最小所需容量就变成了10，否则就是真正需要的容量
  
  - 得到最小扩容量后，调用ensurecapacityinternal()方法，用来判断是否需要扩容，如果这个容量比原列表长度大，那么就需要扩容
  
  **先来看 `add` 方法**
  
  ```java
      /**
       * 将指定的元素追加到此列表的末尾。
       */
      public boolean add(E e) {
     //添加元素之前，先调用ensureCapacityInternal方法
        ensureCapacityInternal(size + 1);  // Increments modCount!!
          //这里看到ArrayList添加元素的实质就相当于为数组赋值
        elementData[size++] = e;
          return true;
    }
  ```

  > **注意** ：JDK11 移除了 `ensureCapacityInternal()` 和 `ensureExplicitCapacity()` 方法
  
  **再来看看 `ensureCapacityInternal()` 方法**
  
  （JDK7）可以看到 `add` 方法 首先调用了`ensureCapacityInternal(size + 1)`
  
  ```java
     //得到最小需要容量
      private void ensureCapacityInternal(int minCapacity) {
          if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) {
                // 获取默认的容量和传入参数的较大值
            minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity);
          }

          ensureExplicitCapacity(minCapacity);
    }
  ```

  **当 要 add 进第 1 个元素时，minCapacity 为 1，在 Math.max()方法比较后，minCapacity 为 10。**

  > 此处和后续 JDK8 代码格式化略有不同，核心代码基本一样。
  
  **`ensureExplicitCapacity()` 方法**
  
  如果调用 `ensureCapacityInternal()` 方法就一定会进入（执行）这个方法，下面我们来研究一下这个方法的源码！
  
  ```java
    //判断是否需要扩容
      private void ensureExplicitCapacity(int minCapacity) {
          modCount++;
  
        // overflow-conscious code
          if (minCapacity - elementData.length > 0)
            //调用grow方法进行扩容，调用此方法代表已经开始扩容了
              grow(minCapacity);
    }
  ```

  - 当我们要 add 进第 1 个元素到 `ArrayList` 时，`elementData.length` 为 0 （因为还是一个空的 list），因为执行了 `ensureCapacityInternal()` 方法 ，所以 `minCapacity` 此时为 10。此时，`minCapacity - elementData.length > 0`成立，所以会进入 `grow(minCapacity)` 方法。

  - 当 add 第 1 个元素时，`oldCapacity` 为 0，经比较后第一个 if 判断成立，`newCapacity = minCapacity`(为 10)。但是第二个 if 判断不会成立，即 `newCapacity` 不比 `MAX_ARRAY_SIZE` 大，则不会进入 `hugeCapacity` 方法。数组容量为 10，add 方法中 return true,size 增为 1。

  - 当 add 第 2 个元素时，`minCapacity` 为 2，此时 `elementData.length`(容量)在添加第一个元素后扩容成 10 了。此时，`minCapacity - elementData.length > 0` 不成立，所以不会进入 （执行）`grow(minCapacity)` 方法。

  - 添加第 3、4···到第 10 个元素时，依然不会执行 grow 方法，数组容量都为 10。
  
  - 直到添加第 11 个元素，`minCapacity`(为 11)比 `elementData.length`（为 10）要大。进入 `grow` 方法进行扩容。
  
    **`grow()` 方法**
  
    ```java
        /**
         * 要分配的最大数组大小
         * 有些虚拟机在数组中保留了一些头信息，避免内存溢出，防止请求的数组大小超过虚拟机限制
         */
        private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;
    
        /**
         * ArrayList扩容的核心方法。
         */
        private void grow(int minCapacity) {
            // oldCapacity为旧容量，newCapacity为新容量
            int oldCapacity = elementData.length;
            //将oldCapacity 右移一位，其效果相当于oldCapacity /2，
            //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍，
            int newCapacity = oldCapacity + (oldCapacity >> 1);
            //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量，
            if (newCapacity - minCapacity < 0)
                newCapacity = minCapacity;
           // 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) `hugeCapacity()` 方法来比较 minCapacity 和 MAX_ARRAY_SIZE，
           //如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8`。
          if (newCapacity - MAX_ARRAY_SIZE > 0)
                newCapacity = hugeCapacity(minCapacity);
          // minCapacity is usually close to size, so this is a win:
            elementData = Arrays.copyOf(elementData, newCapacity);
      }
    ```
  
    #### `hugeCapacity()` 方法
  
    从上面 `grow()` 方法源码我们知道： 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) `hugeCapacity()` 方法来比较 minCapacity 和 MAX_ARRAY_SIZE，如果 minCapacity 大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8`。
  
    ```java
        private static int hugeCapacity(int minCapacity) {
            if (minCapacity < 0) // overflow
                throw new OutOfMemoryError();
            //对minCapacity和MAX_ARRAY_SIZE进行比较
            //若minCapacity大，将Integer.MAX_VALUE作为新数组的大小
            //若MAX_ARRAY_SIZE大，将MAX_ARRAY_SIZE作为新数组的大小
          //MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;
            return (minCapacity > MAX_ARRAY_SIZE) ?
              Integer.MAX_VALUE :
                MAX_ARRAY_SIZE;
      }
    ```
  
    **扩容方法**（`grow()` + `hugeCapacity()`）：

    `int newCapacity = oldCapacity + (oldCapacity >> 1)`,所以 `ArrayList` **每次扩容之后容量都会变为原来的 1.5 倍左右**（`oldCapacity` 为偶数就是 1.5 倍，否则是 1.5 倍左右）！奇偶不同，比如 ：10+10/2 = 15, 33+33/2=49。如果是奇数的话会丢掉小数。从 `grow()` 方法源码我们知道： 如果新容量大于 `MAX_ARRAY_SIZE`,进入(执行) `hugeCapacity()` 方法来比较 `minCapacity` 和 `MAX_ARRAY_SIZE`，如果 `minCapacity` 大于最大容量，则新容量则为**`Integer.MAX_VALUE`**，否则，新容量大小则为 `MAX_ARRAY_SIZE` 即为 **`Integer.MAX_VALUE - 8`**。

    ```
    ">>"（移位运算符）：>>1 右移一位相当于除 2，右移 n 位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了 1 位所以相当于 oldCapacity /2。对于大数据的 2 进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源
    ```
  
    **我们再来通过例子探究一下`grow()` 方法 ：**
  
    - 当 add 第 1 个元素时，oldCapacity 为 0，经比较后第一个 if 判断成立，newCapacity = minCapacity(为 10)。但是第二个 if 判断不会成立，即 newCapacity 不比 MAX_ARRAY_SIZE 大，则不会进入 `hugeCapacity` 方法。数组容量为 10，add 方法中 return true,size 增为 1。
    - 当 add 第 11 个元素进入 grow 方法时，newCapacity 为 15，比 minCapacity（为 11）大，第一个 if 判断不成立。新容量没有大于数组最大 size，不会进入 hugeCapacity 方法。数组容量扩为 15，add 方法中 return true,size 增为 11。
    - 以此类推······

####[1.2.3.3. `System.arraycopy()` 和 `Arrays.copyOf()`方法](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/ArrayList源码+扩容机制分析?id=_33-systemarraycopy-和-arrayscopyof方法)

**联系：**

看两者源代码可以发现 `copyOf()`内部实际调用了 `System.arraycopy()` 方法

**区别：**

`arraycopy()` 需要目标数组，将原数组拷贝到你自己定义的数组里或者原数组，而且可以选择拷贝的起点和长度以及放入新数组中的位置 `copyOf()` 是系统自动在内部新建一个数组，并返回该数组

#### [1.2.3.4. `ensureCapacity`方法](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/ArrayList源码+扩容机制分析?id=_34-ensurecapacity方法)

**最好在 add 大量元素之前用 `ensureCapacity` 方法，以减少增量重新分配的次数**

可以提高`ArrayList`调用的速度

## [1.3. Collection 子接口之 Set](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_13-collection-子接口之-set)

### [1.3.1. comparable 和 Comparator 的区别](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_131-comparable-和-comparator-的区别)

- `comparable` 接口实际上是出自`java.lang`包 它有一个 `compareTo(Object obj)`方法用来排序
- `comparator`接口实际上是出自 `java.util` 包它有一个`compare(Object obj1, Object obj2)`方法用来排序

一般我们需要对一个集合使用自定义排序时，我们就要**重写**`compareTo()`方法或`compare()`方法，当我们需要对某一个集合实现两种排序方式，比如一个 song 对象中的歌名和歌手名分别采用一种排序方法的话，我们可以重写`compareTo()`方法和使用自制的`Comparator`方法或者以两个 Comparator 来实现歌名排序和歌星名排序，第二种代表我们只能使用两个参数版的 `Collections.sort()`.

#### Comparator 定制排序

```java
Collections.sort(arrayList, new Comparator<Integer>() {
            @Override
            public int compare(Integer o1, Integer o2) {
                return o2.compareTo(o1);
                // return o2 - o1;
            }
        });
```

#### 重写 compareTo 方法实现按年龄来排序

```java
// person对象没有实现Comparable接口，所以必须实现，这样才不会出错，才可以使treemap中的数据按顺序排列
// 前面一个例子的String类已经默认实现了Comparable接口，详细可以查看String类的API文档，另外其他
// 像Integer类等都已经实现了Comparable接口，所以不需要另外实现了
public  class Person implements Comparable<Person> {
	/**
     * T重写compareTo方法实现按年龄来排序
     */
    @Override
    public int compareTo(Person o) {
        if (this.age > o.getAge()) {
            return 1;
        }
        if (this.age < o.getAge()) {
            return -1;
        }
        return 0;
    }
}
```

### [1.3.2. 无序性和不可重复性的含义是什么](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_132-无序性和不可重复性的含义是什么)

1、什么是无序性？无序性不等于随机性 ，无序性是指存储的数据在底层数组中并非按照数组索引的顺序添加 ，而是根据数据的哈希值决定的。

2、什么是不可重复性？不可重复性是指添加的元素按照 equals()判断时 ，返回 false，需要同时重写 equals()方法和 HashCode()方法。

### [1.3.3. 比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_133-比较-hashset、linkedhashset-和-treeset-三者的异同)

`HashSet` 是 `Set` 接口的主要实现类 ，`HashSet` 的底层是 `HashMap`，**线程不安全的，可以存储 null 值**；

`LinkedHashSet` 是 `HashSet` 的子类，能够**按照添加的顺序遍历**；

`TreeSet` 底层使用**红黑树**，能够**按照添加元素的顺序进行遍历**，排序的方式有**自然排序和定制排序**。

### [1.3.4. HashSet 如何检查重复](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_144-hashset-如何检查重复)

当你把对象加入`HashSet`时，

- `HashSet` 会先计算对象的`hashcode`值来判断对象加入的位置，同时也会与其他加入的对象的 `hashcode` 值作比较，如果没有相符的 `hashcode`，`HashSet` 会假设对象没有重复出现。
- 但是如果发现有相同 `hashcode` 值的对象，这时会调用`equals()`方法来检查 `hashcode` 相等的对象是否真的相同。
- 如果两者相同，`HashSet` 就不会让加入操作成功。

**`hashCode()`与 `equals()` 的相关规定：**

1. 如果两个对象相等，则 `hashcode` 一定也是相同的
2. 两个对象相等,对两个 `equals()` 方法返回 true
3. 两个对象有相同的 `hashcode` 值，它们也不一定是相等的
4. 综上，`equals()` 方法被覆盖过，则 `hashCode()` 方法也必须被覆盖
5. `hashCode() `的默认行为是对堆上的对象产生独特值。如果没有重写 `hashCode()`，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。

**== 与 equals 的区别**

1. 对于基本类型来说，== 比较的是值是否相等；
2. 对于引用类型来说，== 比较的是两个引用是否指向同一个对象地址（两者在内存中存放的地址（堆内存地址）是否指向同一个地方）；
3. 对于引用类型（包括包装类型）来说，equals 如果没有被重写，对比它们的地址是否相等；如果 equals()方法被重写（例如 String），则比较的是地址里的内容。

##### 为什么在`hashmap`或`hashset`中存放自定义类型的对象时需要重写`hashcode()`和`equals()`方法

​	    如果在`HashSet`和`HashMap`的键中存放的自定义类型，需要重写Object里的同名方法`hashcode()`和`equals()`。

​		如果不重写：

​		当我们往`HashMap`里放`k1`时，首先会调用Key这个类的`hashCode`方法计算它的hash值，随后把`k1`放入hash值所指引的内存位置。关键是我们没有在Key里定义`hashCode`方法。这里**调用的仍是`Object`类的`hashCode`方法**（所有的类都是Object的子类），而`Object`类的`hashCode`方法返回的**`hash`值其实是`k1`对象的内存地址**（假设是1000）。

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\1226172-20190305064746385-445045.png)

​		如果我们随后是调用`hm.get(k1)`，那么我们会再次调用`hashCode`方法（还是返回`k1`的地址1000），随后根据得到的hash值，能很快地找到`k1`。但我们这里的代码是`hm.get(k2)`，当我们调用Object类的`hashCode`方法（因为Key里没定义）计算`k2`的hash值时，其实得到的是`k2`的内存地址（假设是2000）。由于`k1`和`k2`是两个不同的对象，所以它们的**内存地址一定不会相同**，也就是说它们的hash值一定不同，这就是我们无法用`k2`的hash值去拿`k1`的原因。

​		如果重写`hashCode`方法以后，存`k1`时，是根据它id的hash值，假设这里是100，把`k1`对象放入到对应的位置。而取`k2`时，是先计算它的hash值（由于`k2`的id也是1，这个值也是100），随后到这个位置去找。 但结果会出乎我们意料：明明100号位置已经有`k1`，但第26行的输出结果依然是null。其原因就是**没有重写Key对象的equals方法。**

​		`HashMap`是用**链地址法**来处理冲突，也就是说，在100号位置上，有可能存在着多个用链表形式存储的对象。它们通过`hashCode`方法返回的hash值都是100。

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\1226172-20190305064916326-355335633.png)

​		当我们通过`k2`的`hashCode`到100号位置查找时，确实会得到`k1`。但`k1`有可能仅仅是和`k2`具有相同的hash值，但未必和`k2`相等（`k1`和`k2`两把钥匙未必能开同一扇门），这个时候，就需要调用Key对象的equals方法来判断两者是否相等了。

  	由于我们在Key对象里没有定义equals方法，系统就不得不调用Object类的equals方法。由于Object的固有方法是根据两个对象的内存地址来判断，所以`k1`和`k2`一定不会相等，这就是为什么依然通过`hm.get(k2)`依然得到null的原因。

> 总结：`hashcode`和`equals`不写就会去调用object类的同名方法，它们都是去比较对象的内存地址，即便`hashcode`一样，也不一定返回相等的结果，应为链式存储的原因，同样的地址回存放不同的对象。

## [1.4. Map 接口](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_14-map-接口)

### [1.4.1. HashMap 和 Hashtable 的区别](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_141-hashmap-和-hashtable-的区别)

1. **线程是否安全：** `HashMap` 是非线程安全的，`HashTable` 是线程安全的,因为 `HashTable` 内部的方法基本都经过`synchronized` 修饰。（如果你要保证线程安全的话就使用 `ConcurrentHashMap` 吧！）；

2. **效率：** 因为线程安全的问题，`HashMap` 要比 `HashTable` 效率高一点。另外，`HashTable` 基本被淘汰，不要在代码中使用它；

3. **对 Null key 和 Null value 的支持：** `HashMap` 可以存储 null 的 key 和 value，但 null 作为键只能有一个，null 作为值可以有多个；`HashTable` 不允许有 null 键和 null 值，否则会抛出 `NullPointerException`。

4. **初始容量大小和每次扩充容量大小的不同 ：**

   ① 创建时如果不指定容量初始值，`Hashtable` 默认的初始大小为 11，之后每次扩充，容量变为原来的 `2n+1`。**`HashMap` 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。**

   ② 创建时如果给定了容量初始值，那么 `Hashtable` 会直接使用你给定的大小，而 `HashMap` 会将其扩充为 2 的幂次方大小（`HashMap` 中的`tableSizeFor()`方法保证，下面给出了源代码）。也就是说 `HashMap` 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方。

5. **底层数据结构：** `JDK1.8` 以后的 `HashMap` 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。`Hashtable` 没有这样的机制。

### [1.4.2. HashMap 和 HashSet 区别](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_142-hashmap-和-hashset-区别)

如果你看过 `HashSet` 源码的话就应该知道：`HashSet` 底层就是基于 `HashMap` 实现的。（`HashSet` 的源码非常非常少，因为除了 `clone()`、`writeObject()`、`readObject()`是 `HashSet` 自己不得不实现之外，其他方法都是直接调用 `HashMap` 中的方法。

| `HashMap`                              | `HashSet`                                                    |
| -------------------------------------- | ------------------------------------------------------------ |
| 实现了 `Map` 接口                      | 实现 `Set` 接口                                              |
| 存储键值对                             | 仅存储对象                                                   |
| 调用 `put()`向 map 中添加元素          | 调用 `add()`方法向 `Set` 中添加元素                          |
| `HashMap` 使用键（Key）计算 `hashcode` | `HashSet` 使用成员对象来计算 `hashcode` 值，对于两个对象来说 `hashcode` 可能相同，所以` equals()`方法用来判断对象的相等性 |

### [1.4.3. HashMap 和 TreeMap 区别](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_143-hashmap-和-treemap-区别)

`TreeMap` 和`HashMap` 都继承自`AbstractMap` ，但是需要注意的是`TreeMap`它还实现了`NavigableMap`接口和`SortedMap` 接口。

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\TreeMap继承结构.png)

实现 `NavigableMap` 接口让 `TreeMap` 有了对集合内元素的搜索的能力。

实现`SortMap`接口让 `TreeMap` 有了对集合中的元素根据键排序的能力。默认是按 key 的升序排序，不过我们也可以指定排序的比较器。

```java
/**
 * @author shuang.kou
 * @createTime 2020年06月15日 17:02:00
 */
public class Person {
    private Integer age;

    public Person(Integer age) {
        this.age = age;
    }

    public Integer getAge() {
        return age;
    }


    public static void main(String[] args) {
        TreeMap<Person, String> treeMap = new TreeMap<>(new Comparator<Person>() {
            @Override
            public int compare(Person person1, Person person2) {
                int num = person1.getAge() - person2.getAge();
                return Integer.compare(num, 0);
            }
        });
        treeMap.put(new Person(3), "person1");
        treeMap.put(new Person(18), "person2");
        treeMap.put(new Person(35), "person3");
        treeMap.put(new Person(16), "person4");
        treeMap.entrySet().stream().forEach(personStringEntry -> {
            System.out.println(personStringEntry.getValue());
        });
    }
}

输出：
person1
person4
person2
person3
```

可以看出，`TreeMap` 中的元素已经是按照 `Person` 的 age 字段的升序来排列了。

上面，我们是通过传入匿名内部类的方式实现的，你可以将代码替换成 Lambda 表达式实现的方式：

```java
TreeMap<Person, String> treeMap = new TreeMap<>((person1, person2) -> {
  int num = person1.getAge() - person2.getAge();
  return Integer.compare(num, 0);
});
```

**综上，相比于`HashMap`来说 `TreeMap` 主要多了对集合中的元素根据键排序的能力以及对集合内元素的搜索的能力。**

### [1.4.4. HashMap 的底层实现](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_145-hashmap-的底层实现)

#### 1.4.4.1 底层数据结构

**[JDK1.8 之前](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_1451-jdk18-之前)**

`JDK1.8` 之前 `HashMap` 底层是 **数组和链表** 结合在一起使用也就是 **链表散列**。`HashMap` 通过 key 的 `hashCode` 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) & hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。

所谓扰动函数指的就是 `HashMap` 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 `hashCode()` 方法，换句话说**使用扰动函数之后可以减少碰撞**。

**`JDK 1.8` `HashMap` 的 hash 方法源码:**

`JDK 1.8` 的 hash 方法 相比于 `JDK 1.7` hash 方法更加简化，但是原理不变。

```java
    static final int hash(Object key) {
      int h;
      // key.hashCode()：返回散列值也就是hashcode
      // ^ ：按位异或
      // >>>:无符号右移，忽略符号位，空位都以0补齐
      return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
  }
```

​		如果不进行扰动，那么在数组长度很小的时候，即便散列值再松散，只要取后几位的话，碰撞问题还是很严重，要是散列本身做得不好，分布上成等差数列的漏洞，恰好使最后几个低位呈现规律性重复，会很麻烦。

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\4acf898694b8fb53498542dc0c5f765a_r.jpg)

​		右位移16位，正好是`32bit`的一半，自己的高半区和低半区做异或，就是为了混合原始哈希码的高位和低位，以此来加大低位的随机性。而且混合后的低位参杂了高位的部分特征，这样高位的信息也被变相保留下来

**`JDK1.7` 的 `HashMap` 的 hash 方法源码：**

```java
static int hash(int h) {
    // This function ensures that hashCodes that differ only by
    // constant multiples at each bit position have a bounded
    // number of collisions (approximately 8 at default load factor).

    h ^= (h >>> 20) ^ (h >>> 12);
    return h ^ (h >>> 7) ^ (h >>> 4);
}Copy to clipboardErrorCopied
```

相比于 `JDK1.8` 的 hash 方法 ，`JDK 1.7` 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。

所谓 **“拉链法”** 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。

![jdk1.8之前的内部结构](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\jdk1.8之前的内部结构.png)

[**JDK1.8 之后**](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_1452-jdk18-之后)

当链表长度大于阈值（默认为 8）时，会首先调用 `treeifyBin()`方法。这个方法会根据 `HashMap` 数组来决定是否转换为红黑树。只有当数组长度大于或者等于 64 的情况下，才会执行转换红黑树操作，以减少搜索时间。否则，就是只是执行 `resize()` 方法对数组扩容。重点关注 `treeifyBin()`方法即可。

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\up-bba283228693dae74e78da1ef7a9a04c684.png)

> `TreeMap`、`TreeSet` 以及 `JDK1.8` 之后的 `HashMap` 底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为**二叉查找树在某些情况下会退化成一个线性结构**。

**`HashMap`类的属性**

```java
public class HashMap<K,V> extends AbstractMap<K,V> implements Map<K,V>, Cloneable, Serializable {
    // 序列号
    private static final long serialVersionUID = 362498820763181265L;
    // 默认的初始容量是16
    static final int DEFAULT_INITIAL_CAPACITY = 1 << 4;
    // 最大容量
    static final int MAXIMUM_CAPACITY = 1 << 30;
    // 默认的填充因子
    static final float DEFAULT_LOAD_FACTOR = 0.75f;
    // 当桶(bucket)上的结点数大于这个值时会转成红黑树
    static final int TREEIFY_THRESHOLD = 8;
    // 当桶(bucket)上的结点数小于这个值时树转链表
    static final int UNTREEIFY_THRESHOLD = 6;
    // 桶中结构转化为红黑树对应的table的最小大小
    static final int MIN_TREEIFY_CAPACITY = 64;
    // 存储元素的数组，总是2的幂次倍
    transient Node<k,v>[] table;
    // 存放具体元素的集
    transient Set<map.entry<k,v>> entrySet;
    // 存放元素的个数，注意这个不等于数组的长度。
    transient int size;
    // 每次扩容和更改map结构的计数器
    transient int modCount;
    // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容
    int threshold;
    // 加载因子
    final float loadFactor;
}
```

> **loadFactor 加载因子**
>
> loadFactor 加载因子是控制数组存放数据的疏密程度，loadFactor 越趋近于 1，那么 数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加，loadFactor 越小，也就是趋近于 0，数组中存放的数据(entry)也就越少，也就越稀疏。
>
> **loadFactor 太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor 的默认值为 0.75f 是官方给出的一个比较好的临界值**。
>
> 给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。
>
> **threshold**
>
> **threshold = capacity \* `loadFactor`**，**当 Size>=threshold**的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 **衡量数组是否需要扩增的一个标准**。
>
> 结合负载因子的定义公式可知，threshold就是在此Load factor和length(数组长度)对应下允许的最大元素数目，超过这个数目就重新resize(扩容)，扩容后的`HashMap`容量是之前容量的两倍。**默认的负载因子0.75是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子Load factor的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子`loadFactor`的值，这个值可以大于1。**

**Node 节点类源码:**

```java
// 继承自 Map.Entry<K,V>
static class Node<K,V> implements Map.Entry<K,V> {
       final int hash;// 哈希值，存放元素到hashmap中时用来与其他元素hash值比较
       final K key;//键
       V value;//值
       // 指向下一个节点
       Node<K,V> next;
       Node(int hash, K key, V value, Node<K,V> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }
        public final K getKey()        { return key; }
        public final V getValue()      { return value; }
        public final String toString() { return key + "=" + value; }
        // 重写hashCode()方法
        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }

        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }
        // 重写 equals() 方法
        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Map.Entry<?,?> e = (Map.Entry<?,?>)o;
                if (Objects.equals(key, e.getKey()) &&
                    Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
}
```

需要重写hashcode() 和equals()方法



**树节点类源码:**

```java
static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V> {
        TreeNode<K,V> parent;  // 父
        TreeNode<K,V> left;    // 左
        TreeNode<K,V> right;   // 右
        TreeNode<K,V> prev;    // needed to unlink next upon deletion
        boolean red;           // 判断颜色
        TreeNode(int hash, K key, V val, Node<K,V> next) {
            super(hash, key, val, next);
        }
        // 返回根节点
        final TreeNode<K,V> root() {
            for (TreeNode<K,V> r = this, p;;) {
                if ((p = r.parent) == null)
                    return r;
                r = p;
       		}
      	}
}
```

#### [1.4.4.2HashMap 源码分析](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/HashMap(JDK1.8)源码+底层数据结构分析?id=hashmap-源码分析)

##### [构造方法](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/HashMap(JDK1.8)源码+底层数据结构分析?id=构造方法)

`HashMap` 中有四个构造方法，它们分别如下：

```java
    // 默认构造函数。
    public HashMap() {
        this.loadFactor = DEFAULT_LOAD_FACTOR; // all   other fields defaulted
     }

     // 包含另一个“Map”的构造函数
     public HashMap(Map<? extends K, ? extends V> m) {
         this.loadFactor = DEFAULT_LOAD_FACTOR;
         putMapEntries(m, false);//下面会分析到这个方法
     }

     // 指定“容量大小”的构造函数
     public HashMap(int initialCapacity) {
         this(initialCapacity, DEFAULT_LOAD_FACTOR);
     }

     // 指定“容量大小”和“加载因子”的构造函数
     public HashMap(int initialCapacity, float loadFactor) {
         if (initialCapacity < 0)
             throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity);
         if (initialCapacity > MAXIMUM_CAPACITY)
             initialCapacity = MAXIMUM_CAPACITY;
         if (loadFactor <= 0 || Float.isNaN(loadFactor))
             throw new IllegalArgumentException("Illegal load factor: " + loadFactor);
         this.loadFactor = loadFactor;
         this.threshold = tableSizeFor(initialCapacity);
     }
```

**putMapEntries 方法：**

```java
final void putMapEntries(Map<? extends K, ? extends V> m, boolean evict) {
    int s = m.size();
    if (s > 0) {
        // 判断table是否已经初始化
        if (table == null) { // pre-size
            // 未初始化，s为m的实际元素个数
            float ft = ((float)s / loadFactor) + 1.0F;
            int t = ((ft < (float)MAXIMUM_CAPACITY) ?
                    (int)ft : MAXIMUM_CAPACITY);
            // 计算得到的t大于阈值，则初始化阈值
            if (t > threshold)
                threshold = tableSizeFor(t);
        }
        // 已初始化，并且m元素个数大于阈值，进行扩容处理
        else if (s > threshold)
            resize();
        // 将m中的所有元素添加至HashMap中
        for (Map.Entry<? extends K, ? extends V> e : m.entrySet()) {
            K key = e.getKey();
            V value = e.getValue();
            putVal(hash(key), key, value, false, evict);
        }
    }
}
```

#####[**put 方法(扩容机制)**](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/HashMap(JDK1.8)源码+底层数据结构分析?id=put-方法)

`HashMap` 只提供了 put 用于添加元素，`putVal` 方法只是给 put 方法调用的一个方法，并没有提供给用户使用。

**对 `putVal` 方法添加元素的分析如下：**

1. 如果定位到的数组位置没有元素 就直接插入。
2. 如果定位到的数组位置有元素就和要插入的 key 比较，如果 key 相同就直接覆盖，如果 key 不相同，就判断 p 是否是一个树节点，如果是就调用`e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value)`将元素添加进入。如果不是就遍历链表插入(插入的是链表**尾部**)。尾插法

![](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\58e67eae921e4b431782c07444af824e_r.jpg)

> ①.判断键值对数组table[i]是否为空或为null，是则执行resize()进行扩容；
>
> ②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③；
>
> ③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals；
>
> ④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤；
>
> ⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可；
>
> ⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。

```java
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}

final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    // table未初始化或者长度为0，进行扩容
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    // (n - 1) & hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中)
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);
    // 桶中已经存在元素
    else {
        Node<K,V> e; K k;
        // 比较桶中第一个元素(数组中的结点)的hash值相等，key相等
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
                // 将第一个元素赋值给e，用e来记录
                e = p;
        // hash值不相等，即key不相等；为红黑树结点
        else if (p instanceof TreeNode)
            // 放入树中
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        // 为链表结点
        else {
            // 在链表最末插入结点
            for (int binCount = 0; ; ++binCount) {
                // 到达链表的尾部
                if ((e = p.next) == null) {
                    // 在尾部插入新结点
                    p.next = newNode(hash, key, value, null);
                    // 结点数量达到阈值(默认为 8 )，执行 treeifyBin 方法
                    // 这个方法会根据 HashMap 数组来决定是否转换为红黑树。
                    // 只有当数组长度大于或者等于 64 的情况下，才会执行转换红黑树操作，以减少搜索时间。否则，就是只是对数组扩容。
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    // 跳出循环
                    break;
                }
                // 判断链表中结点的key值与插入的元素的key值是否相等
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    // 相等，跳出循环
                    break;
                // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表
                p = e;
            }
        }
        // 表示在桶中找到key值、hash值与插入元素相等的结点
        if (e != null) {
            // 记录e的value
            V oldValue = e.value;
            // onlyIfAbsent为false或者旧值为null
            if (!onlyIfAbsent || oldValue == null)
                //用新值替换旧值
                e.value = value;
            // 访问后回调
            afterNodeAccess(e);
            // 返回旧值
            return oldValue;
        }
    }
    // 结构性修改
    ++modCount;
    // 实际大小大于阈值则扩容
    if (++size > threshold)
        resize();
    // 插入后回调
    afterNodeInsertion(evict);
    return null;
}
```

**我们再来对比一下 JDK1.7 put 方法的代码**

**对于 put 方法的分析如下：**

- ① 如果定位到的数组位置没有元素 就直接插入。
- ② 如果定位到的数组位置有元素，遍历以这个元素为头结点的链表，依次和插入的 key 比较，如果 key 相同就直接覆盖，不同就采用头插法插入元素。

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\v2-4553f2400ec0df95cf4174660298e797_r.jpg)

```java
public V put(K key, V value)
    if (table == EMPTY_TABLE) {
    inflateTable(threshold);
}
    if (key == null)
        return putForNullKey(value);
    int hash = hash(key);
    int i = indexFor(hash, table.length);
    for (Entry<K,V> e = table[i]; e != null; e = e.next) { // 先遍历
        Object k;
        if (e.hash == hash && ((k = e.key) == key || key.equals(k))) {
            V oldValue = e.value;
            e.value = value;
            e.recordAccess(this);
            return oldValue;
        }
    }

    modCount++;
    addEntry(hash, key, value, i);  // 再插入
    return null;
}
```

#####[**get 方法**](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/HashMap(JDK1.8)源码+底层数据结构分析?id=get-方法)

```java
public V get(Object key) {
    Node<K,V> e;
    return (e = getNode(hash(key), key)) == null ? null : e.value;
}

final Node<K,V> getNode(int hash, Object key) {
    Node<K,V>[] tab; Node<K,V> first, e; int n; K k;
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (first = tab[(n - 1) & hash]) != null) {
        // 数组元素相等
        if (first.hash == hash && // always check first node
            ((k = first.key) == key || (key != null && key.equals(k))))
            return first;
        // 桶中不止一个节点
        if ((e = first.next) != null) {
            // 在树中get
            if (first instanceof TreeNode)
                return ((TreeNode<K,V>)first).getTreeNode(hash, key);
            // 在链表中get
            do {
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    return e;
            } while ((e = e.next) != null);
        }
    }
    return null;
}
```

计算key的hash值，得出在数组中桶的编号，判断与头节点是否相等，如果相等，直接返回头节点的值

如果不相等，先判断是否为树节点，如果是，那么在树中查找这此节点的值，如果不是树节点就在链表中查找该节点的值

#####[**resize 方法**](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/HashMap(JDK1.8)源码+底层数据结构分析?id=resize-方法)

扩容(resize)就是重新计算容量，向`HashMap`对象里不停的添加元素，而`HashMap`对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。

进行扩容，会伴随着一次重新 hash 分配，并且会遍历 hash 表中所有的元素，是非常耗时的。在编写程序中，要尽量避免 resize。

```java
final Node<K,V>[] resize() {
    Node<K,V>[] oldTab = table;
    int oldCap = (oldTab == null) ? 0 : oldTab.length;
    int oldThr = threshold;
    int newCap, newThr = 0;
    if (oldCap > 0) {// oldCap = 0，oldThr = 0
        // 超过最大值就不再扩充了，就只好随你碰撞去吧
        if (oldCap >= MAXIMUM_CAPACITY) {
            threshold = Integer.MAX_VALUE;
            return oldTab;
        }
        // 没超过最大值，并且旧的容量大于初始容量，就扩充为原来的2倍，如果没超过初始容量就不扩容
        else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY && oldCap >= DEFAULT_INITIAL_CAPACITY)
            newThr = oldThr << 1; // 扩容阈值提高为原来的两倍
    }
    else if (oldThr > 0) {
        // oldCap = 0，oldThr > 0，那么初始容量即等于oldThr=12
        // 带参初始化会进入这里,主要是为了重新算threshold
        newCap = oldThr;
    else {// oldCap = 0，oldThr = 0，不带参初始化会进入这里
        newCap = DEFAULT_INITIAL_CAPACITY;
        newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
    }
    // 计算新的resize上限
    if (newThr == 0) {
        float ft = (float)newCap * loadFactor;
        newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE);
    }
    threshold = newThr;
    @SuppressWarnings({"rawtypes","unchecked"})
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    table = newTab;
    if (oldTab != null) {
        // 把每个bucket都移动到新的buckets中
        for (int j = 0; j < oldCap; ++j) {
            Node<K,V> e;
            if ((e = oldTab[j]) != null) {
                oldTab[j] = null;
                if (e.next == null)
                    newTab[e.hash & (newCap - 1)] = e;
                else if (e instanceof TreeNode)
                    ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                else {
                    Node<K,V> loHead = null, loTail = null;
                    Node<K,V> hiHead = null, hiTail = null;
                    Node<K,V> next;
                    do {
                        next = e.next;
                        // 原索引
                        if ((e.hash & oldCap) == 0) {
                            if (loTail == null)
                                loHead = e;
                            else
                                loTail.next = e;
                            loTail = e;
                        }
                        // 原索引+oldCap
                        else {
                            if (hiTail == null)
                                hiHead = e;
                            else
                                hiTail.next = e;
                            hiTail = e;
                        }
                    } while ((e = next) != null);
                    // 原索引放到bucket里
                    if (loTail != null) {
                        loTail.next = null;
                        newTab[j] = loHead;
                    }
                    // 原索引+oldCap放到bucket里
                    if (hiTail != null) {
                        hiTail.next = null;
                        newTab[j + oldCap] = hiHead;
                    }
                }
            }
        }
    }
    return newTab;
}
```

元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)：

因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图：

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\2019021916223252.png)

这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。

### [1.4.5. HashMap 的长度为什么是 2 的幂次方](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_146-hashmap-的长度为什么是-2-的幂次方)

​		为了能让 `HashMap` 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀。我们上面也讲到了过了，Hash 值的范围值-2147483648 到 2147483647，前后加起来大概 40 亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是**一个 40 亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标**。这个数组下标的计算方法是“ `(n - 1) & hash`”。（n 代表数组长度）。这也就解释了 `HashMap` 的长度为什么是 2 的幂次方。

>  如果是2的幂次方，那么n-1的二进制全是1111，与”操作的结果就是散列值的高位全部归零，只保留低位值，用来做数组下标访问。

**这个算法应该如何设计呢？**

我们首先可能会想到采用%取余的操作来实现。但是，重点来了：**“取余(%)操作中如果除数是 2 的幂次则等价于与其除数减一的与(&)操作（也就是说 hash%length==hash&(length-1)的前提是 length 是 2 的 n 次方；）。”** 并且 **采用二进制位操作 &，相对于%能够提高运算效率，这就解释了 `HashMap` 的长度为什么是 2 的幂次方。**

### [1.4.6. HashMap 多线程操作导致死循环问题](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_147-hashmap-多线程操作导致死循环问题)

**`JDK1.7`链表插入采用头插法会产生：**

主要原因在于并发下的 `Rehash` 会造成元素之间会形成一个循环链表。不过，`jdk 1.8` 后解决了这个问题，但是还是不建议在多线程下使用 `HashMap`,因为多线程下使用 `HashMap` 还是会存在其他问题比如数据丢失。并发环境下推荐使用 `ConcurrentHashMap` 。

**问题的症状**

程序性能有问题，所以需要变成多线程的，于是，变成多线程后到了线上，发现程序经常占了100%的CPU，查看堆栈，你会发现程序都Hang在了`HashMap.get()`这个方法上了，重启程序后问题消失。但是过段时间又会来。而且，这个问题在测试环境里可能很难重现。

#### 正常的`ReHash`的过程

画了个图做了个演示。

- 我假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。

- 最上面的是old hash 表，其中的Hash表的size=2, 所以key = 3, 7, 5，在mod 2以后都冲突在table[1]这里了。

- 接下来的三个步骤是Hash表 resize成4，然后所有的<key,value> 重新rehash的过程

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\HashMap01.jpg)

#### 并发下的Rehash

**1）假设我们有两个线程。**分别用红色和浅蓝色标注了一下。

我们再回头看一下我们的 transfer代码中的这个细节：

```java
do {
    Entry<K,V> next = e.next; // <--假设线程一执行到这里就被调度挂起了
    int i = indexFor(e.hash, newCapacity);
    e.next = newTable[i];
    newTable[i] = e;
    e = next;
} while (e != null);
```

而我们的线程二执行完成了。于是我们有下面的这个样子。

![img](https://coolshell.cn/wp-content/uploads/2013/05/HashMap02.jpg)

注意，**因为Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表**。我们可以看到链表的顺序被反转后。

**2）线程一被调度回来执行。**

- **先是执行 newTalbe[i] = e;**
- **然后是e = next，导致了e指向了key(7)，**
- **而下一次循环的next = e.next导致了next指向了key(3)**

![img](https://coolshell.cn/wp-content/uploads/2013/05/HashMap03.jpg)

**3）一切安好。**

线程一接着工作。**把key(7)摘下来，放到newTable[i]的第一个，然后把e和next往下移**。

![img](https://coolshell.cn/wp-content/uploads/2013/05/HashMap04.jpg)

**4）环形链接出现。**

**`e.next` = `newTable[i]` 导致 key(3).next 指向了 key(7)**

**注意：此时的kedy(7).next 已经指向了key(3)， 环形链表就这样出现了。**

![img](https://coolshell.cn/wp-content/uploads/2013/05/HashMap05.jpg)

**于是，当我们的线程一调用到，`HashTable.get(11)`时，悲剧就出现了——Infinite Loop。**

### [1.4.7. HashMap 有哪几种常见的遍历方式?](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_148-hashmap-有哪几种常见的遍历方式)

`HashMap` 4 大类（迭代器、for、lambda、stream）遍历方式，以及具体的 7 种遍历方法，**除了 Stream 的并行循环，其他几种遍历方法的性能差别不大，但从简洁性和优雅性上来看，Lambda 和 Stream 无疑是最适合的遍历方式**。除此之外我们还从「安全性」方面测试了 4 大类遍历结果，**从安全性来讲，我们应该使用迭代器提供的 `iterator.remove()` 方法来进行删除，这种方式是安全的在遍历中删除集合的方式，或者使用 Stream 中的 `filter` 过滤掉要删除的数据再进行循环，也是安全的操作方式**。

### [1.4.8. ConcurrentHashMap 和 Hashtable 的区别](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_149-concurrenthashmap-和-hashtable-的区别)

`ConcurrentHashMap` 和 `Hashtable` 的区别主要体现在实现线程安全的方式上不同。

- **底层数据结构：** 

  `JDK1.7` 的 `ConcurrentHashMap` 底层采用 **分段的数组+链表** 实现，`JDK1.8` 采用的数据结构跟 `HashMap1.8` 的结构一样，数组+链表/红黑二叉树。`Hashtable` 和 `JDK1.8` 之前的 `HashMap` 的底层数据结构类似都是采用 **数组+链表** 的形式，数组是 `HashMap` 的主体，**链表则是主要为了解决哈希冲突而存在的**；

- **实现线程安全的方式（重要）：**

  ① **在 `JDK1.7` 的时候，`ConcurrentHashMap`（分段锁）** 对整个桶数组进行了分割分段(`Segment`)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 **到了 `JDK1.8` 的时候已经摒弃了 `Segment` 的概念，而是直接用 `Node` 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 `CAS` 来操作。（`JDK1.6`以后对 `synchronized` 锁做了很多优化）**整个看起来就像是优化过且线程安全的 `HashMap`，虽然在 `JDK1.8` 中还能看到 `Segment` 的数据结构，但是已经简化了属性，只是为了兼容旧版本；

  ② **`Hashtable`(同一把锁)** :使用 `synchronized` 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入**阻塞或轮询**状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。

**两者的对比图：**

**HashTable:**

![HashTable全表锁](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\HashTable全表锁.png)

**`JDK1.7` 的 `ConcurrentHashMap`：**

![JDK1.7的ConcurrentHashMap](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/ConcurrentHashMap%E5%88%86%E6%AE%B5%E9%94%81.jpg)

首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。

**`ConcurrentHashMap` 是由 `Segment` 数组结构和 `HashEntry` 数组结构组成**。

Segment 实现了 `ReentrantLock`,所以 `Segment` 是一种可重入锁，扮演锁的角色。`HashEntry` 用于存储键值对数据。

```java
static class Segment<K,V> extends ReentrantLock implements Serializable {
}Copy to clipboardErrorCopied
```

一个 `ConcurrentHashMap` 里包含一个 `Segment` 数组。`Segment` 的结构和 `HashMap` 类似，是一种数组和链表结构，一个 `Segment` 包含一个 `HashEntry` 数组，每个 `HashEntry` 是一个链表结构的元素，每个 `Segment` 守护着一个 `HashEntry` 数组里的元素，当对 `HashEntry` 数组的数据进行修改时，必须首先获得对应的 `Segment` 的锁。

------

**`JDK1.8` 的 `ConcurrentHashMap`：**

![Java8 ConcurrentHashMap 存储结构（图片来自 javadoop）](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\java8_concurrenthashmap.png)

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X2pwZy9sYUVtaWJIRnhGdzRXaWM3QTRtQW8wUjZFaWNyRHQ3c1NwRXg5RjhtMUllR2liYlM0MGlhV2t4a2liZGVZeXQzQ1N0bGtQcTdGVlFZNGJ6dDdNZnBxQ0hUQmxLQS82NDA)

`JDK1.8` 的 `ConcurrentHashMap` 不在是 **Segment 数组 + `HashEntry` 数组 + 链表**，而是 **Node 数组 + 链表 / 红黑树**。不过，Node 只能用于链表的情况，红黑树的情况需要使用 **`TreeNode`**。当冲突链表达到一定长度时，链表会转换成红黑树。

`ConcurrentHashMap` 取消了 `Segment` 分段锁，采用 `CAS` 和 `synchronized` 来保证并发安全。数据结构跟 `HashMap1.8` 的结构类似，数组+链表/红黑二叉树。JDK 8 在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为 O(N)）(需要判断数组长度是否超过64)转换为红黑树（寻址时间复杂度为 O(log(N))）

`synchronized` **只锁定当前链表或红黑二叉树的首节点，这样只要 hash 不冲突，就不会产生并发，效率又提升 N 倍**。

###**1.4.9. HashMap夺命连环问**

**1.HashMap的底层数据结构是什么？**

```
底层数据结构是哈希表结构（链表散列：数组+单向链表），结合了数组和链表的优点，当链表长度超过8时，且数组长度超过64
链表会转为红黑树。数组中的每一个元素都是链表。总结来说就是HashMap在JDK1.8之前底层是由数组+链表实现的，在JDK1.8开始底层是由数组+链表或者数组+红黑树实现的。
```

**追问：为什么在JDK1.8中进行对HashMap优化的时候，把链表转化为红黑树的阈值是8,而不是7或者不是20呢？**

```
如果选择6和8（如果链表小于等于6树还原转为链表，大于等于8转为树），中间有个差值7可以有效防止链表和树频繁转换。假设一下，如果设计成链表个数超过8则链表转换成树结构，链表个数小于8则树结构转换成链表，如果一个HashMap不停的插入、删除元素，链表个数在8左右徘徊，就会频繁的发生树转链表、链表转树，效率会很低。

还有一点重要的就是由于treenodes的大小大约是常规节点的两倍，因此我们仅在容器包含足够的节点以保证使用时才使用它们，当它们变得太小（由于移除或调整大小）时，它们会被转换回普通的node节点，容器中节点分布在hash桶中的频率遵循泊松分布，桶的长度超过8的概率非常非常小。所以作者应该是根据概率统计而选择了8作为阀值
```

**追问：为什么在1.8中增加红黑树？**

```
当需要查找某个元素的时候，线性探索是最直白的方式，它会把所有数据遍历一遍直到找到你所查找的数据，对于数组和链表这种线性结构来说，当链表长度过长（数据有成百上千）的时候，会造成链表过深的问题，这种查找方式效率极低，时间复杂度是O(n)。简单来说红黑树的出现就是为了提高数据检索的速度。
```

**追问：链表过深问题为什么不用二叉查找树代替，而选择红黑树？为什么不一直使用红黑树？**

```
二叉树在特殊情况下会变成一条线性结构，这就跟原来的链表结构一样了，选择红黑树就是为了解决二叉树的缺陷。

红黑树在插入数据的时候需要通过左旋、右旋、变色这些操作来保持平衡，为了保持这种平衡是需要付出代价的。当链表很短的时候，没必要使用红黑树，否则会导致效率更低，当链表很长的时候，使用红黑树，保持平衡的操作所消耗的资源要远小于遍历链表锁消耗的效率，所以才会设定一个阈值，去判断什么时候使用链表，什么时候使用红黑树。
```

**追问：讲一下你对红黑树的认识**

```
每个节点非红即黑

根节点总是黑色的

如果节点是红色，则它的子节点必须是黑色（反之不一定）

每个叶子节点都是黑色的空节点

从根节点到叶子节点或者空节点的每条路径必须包含相同数量的黑色节点（黑色节点的深度相同）
弱平衡二叉树（由于是弱平衡，可以看到，在相同的节点情况下，AVL树的高度低于红黑树），相对于要求严格的AVL树来说，它的旋转次数少，所以对于搜索，插入，删除操作较多的情况下，我们就用红黑树。
```

**2.讲一下HashMap的工作原理，put()和get()的过程分别是怎么样的？**

```
存储对象时，将key和vaule传给put()方法：

判断数组是否为空，为空进行初始化;

不为空，计算 k 的 hash 值，通过(n - 1) & hash计算应当存放在数组中的下标 index;

查看 table[index] 是否存在数据，没有数据就构造一个Node节点存放在 table[index] 中；

存在数据，说明发生了hash冲突(存在二个节点key的hash值一样), 继续判断key是否相等，相等，用新的value替换原数据(onlyIfAbsent为false)；

如果不相等，判断当前节点类型是不是树型节点，如果是树型节点，创造树型节点插入红黑树中；(如果当前节点是树型节点证明当前已经是红黑树了)

如果不是树型节点，创建普通Node加入链表中；判断链表长度是否大于8并且数组长度大于64，大于的话链表转换为红黑树；

插入完成之后判断当前节点数是否大于阈值（capacity*loadFactor），如果大于开始扩容为原数组的二倍。

下面以流程图方式更加直观的看一下插入流程：


获取对象时，将key传给get()方法：

调用hash(key)方法获取key对应的hash值从而获取该键值对在数组中的下标。

对链表进行顺序遍历，使用equals()方法查找链表中相等的key对应的value值。
```

**追问：说一下数组是怎么扩容的？**

```
使用new HashMap()不传值，默认大小是16，负载因子是0.75。如果传入参数K，那么初始化容量大小为大于K的2的最小整数幂。比如传入的是10，那么初始化容量大小就是16（2的4次方）。

创建一个新数组，新数组初始化容量大小是旧数组的两倍，对原数组中元素重新进行一次hash从而定位在新数组中的存储位置，元素在新数组中的位置只有两种，原下标位置或原下标+旧数组的大小。

追问：为什么要对原数组中元素再重新进行一次hash？直接复制到新数组不行吗？

因为数组长度扩大以后Hash规则也会随之变化。

Hash的公式—> index = HashCode（Key） & （Length - 1）

追问：在插入元素的时候，JDK1.7与JDK1.8有什么不同？

1.7是先判断是否需要扩容，再进行插入操作。1.8是先插入，插入完成之后再判断是否需要扩容。

注：hashcode是用来定位的，定键值对在数组中的存储位置。equals()方法是用来定性的，比较两个对象是否相等。
```

**3.你说JDK1.8之前使用头插法将Entry节点插入链表，那么头插法具体是怎么做的？设计头插法的目的是什么？**

```
设计者认为后来插入的值被查找的概率比较高，使用头插法可以提高查找的效率。新值会作为链表的头部替换原来的值，原来的值会被顺推到链表当中。
```

<img src="C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\image-20210313144411352.png" alt="image-20210313144411352" style="zoom:80%;" />

**4.之前是头插法，为什么JDK1.8之后要改成尾插法？**

```
JDK1.8之前扩容的时候，头插法会导致链表反转，在多线程情况下会出现环形链表，导致取值的时候出现死循环，JDK1.8开始在同样的前提下就不会导致死循环，因为在扩容转移前后链表的顺序不变，保持之前节点的引用关系。
例： A线程和B线程同时向同一个下标位置插入节点，遇到容量不够开始扩容，重新hash，放置元素，采用头插法，后遍历到的B节点放入了头部，这样形成了环，如下图所示：
```

<img src="C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\image-20210313144344643.png" alt="image-20210313144344643" style="zoom:80%;" />

**5.HashMap是怎么设定初始化容量大小的？**

```
使用new HashMap()不传值，默认大小是16，负载因子是0.75。如果传入参数K，那么初始化容量大小为大于K的2的最小整数幂。比如传入的是10，那么初始化容量大小就是16（2的4次方）。
```

**追问：为什么HashMap的数组长度要取2的整数幂？**

```
因为这样数组长度-1正好相当于一个“低位掩码”。“与”操作的结果就是散列值的高位全部归零，只保留低位值，用来做数组下标访问。以初始长度16为例，16-1=15。2进制表示是00000000 00000000 00001111。和某散列值做“与”操作如下，结果就是截取了最低的四位值。
```

**6.讲一下HashMap中的哈希函数时怎么实现的？**

```
key的hashcode是一个32位的int类型值，hash函数就是将hashcode的高16位和低16位进行异或运算。
```

**追问：哈希函数为什么这么设计？**

```
这是一个扰动函数，这样设计的原因主要有两点：

可以最大程度的降低hash碰撞的概率（hash值越分散越好）；

因为是高频操作，所以采用位运算，让算法更加高效；
```

**7.HashMap是线程安全的吗？**

```
不是，在多线程的情况下，1.7的HashMap会导致死循环、数据丢失、数据覆盖。在1.8中如果有多个线程同时put()元素还是会存在数据覆盖的问题。以1.8为例，A线程判断index位置为空后正好挂起，B线程开始向index位置写入节点数据，这时A线程恢复现场，执行赋值操作，就把A线程的数据给覆盖了。
```

**追问：如何解决这个线程不安全的问题？**

```
可以使用HashTable、Collections.synchronizedMap、以及ConcurrentHashMap这些线程安全的Map。

追问：分别讲一下这几种Map都是如何实现线程安全的？

HashTable是直接在操作方法上加synchronized关键字，锁住整个数组，粒度比较大；

Collections.synchronizedMap是使用Collections集合工具的内部类，通过传入Map封装出一个SynchronizedMap对象，内部定义了一个对象锁，方法内通过对象锁实现；

ConcurrentHashMap在JDK1.7中使用分段锁，降低了锁粒度，让并发度大大提高，在JDK 1.8 中直接采用了CAS（无锁算法）+ synchronized的方式来实现线程安全。
```

**8.说一下HashMap在JDK1.8中都有哪些改变？**

```
底层数据结构：1.7中是数组+链表。1.8中是数组+链表或数组+红黑树；

元素插入方式：1.7是头插法插入链表。1.7是尾插法插入链表；

节点类型：1.7中数组中节点类型是Entry节点，1.8中数组中节点类型是Node节点；

元素插入流程：1.7中是先判断是否需要扩容，再插入。1.8中是先插入，插入成功之后再判断是否需要扩容；

扩容方式：1.7中需要对原数组中元素重新进行hash定位在新数组中的位置。1.8中采用更简单的逻辑判断，原下标位置或原下标+旧数组的大小。
```

**9.HashMap的内部节点是有序的吗？**

```
是无序的，根据hash值随机插入。
```

**追问：你知道哪些有序的Map？**

```
LinkedHashMap和TreeMap。
```

**追问：说一下这两种Map分别是怎么实现有序的**

```
LinkedHashMap：LinkedHashMap内部维护了一个单链表，有头尾节点，同时LinkedHashMap节点Entry内部除了继承HashMap的Node属性，还有before 和 after用于标识前置节点和后置节点。可以实现按插入的顺序或访问顺序排序。

TreeHashMap： TreeMap是按照Key的自然顺序或者Comprator的顺序进行排序，内部是通过红黑树来实现。所以要么key所属的类实现Comparable接口，或者自定义一个实现了Comparator接口的比较器，传给TreeMap用于key的比较。
```

**10.HashMap，LinkedHashMap，TreeMap 有什么区别？**

```
LinkedHashMap 保存了记录的插入顺序，在用 Iterator 遍历时，先取到的记录肯定是先插入的；遍历比 HashMap 慢。TreeMap 实现 SortMap 接口，能够把它保存的记录根据键排序（默认按键值升序排序，也可以指定排序的比较器）
```

**追问：讲一下这三种Map的使用场景**

```
一般情况下，使用最多的是 HashMap。

HashMap：在 Map 中插入、删除和定位元素时；

TreeMap：在需要按自然顺序或自定义顺序遍历键的情况下；

LinkedHashMap：在需要输出的顺序和输入的顺序相同的情况下。
```



## [1.5. Collections 工具类](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_15-collections-工具类)

Collections 工具类常用方法:

1. 排序
2. 查找,替换操作
3. 同步控制(不推荐，需要线程安全的集合类型时请考虑使用 JUC 包下的并发集合)

### [1.5.1. 排序操作](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_151-排序操作)

```java
void reverse(List list)//反转
void shuffle(List list)//随机排序
void sort(List list)//按自然排序的升序排序
void sort(List list, Comparator c)//定制排序，由Comparator控制排序逻辑
void swap(List list, int i , int j)//交换两个索引位置的元素
void rotate(List list, int distance)//旋转。当distance为正数时，将list后distance个元素整体移到前面。当distance为负数时，将 list的前distance个元素整体移到后面Copy to clipboardErrorCopied
```

### [1.5.2. 查找,替换操作](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_152-查找替换操作)

```java
int binarySearch(List list, Object key)//对List进行二分查找，返回索引，注意List必须是有序的
int max(Collection coll)//根据元素的自然顺序，返回最大的元素。 类比int min(Collection coll)
int max(Collection coll, Comparator c)//根据定制排序，返回最大元素，排序规则由Comparatator类控制。类比int min(Collection coll, Comparator c)
void fill(List list, Object obj)//用指定的元素代替指定list中的所有元素。
int frequency(Collection c, Object o)//统计元素出现次数
int indexOfSubList(List list, List target)//统计target在list中第一次出现的索引，找不到则返回-1，类比int lastIndexOfSubList(List source, list target).
boolean replaceAll(List list, Object oldVal, Object newVal), 用新元素替换旧元素Copy to clipboardErrorCopied
```

### [1.5.3. 同步控制](https://snailclimb.gitee.io/javaguide/#/docs/java/collection/Java集合框架常见面试题?id=_153-同步控制)

`Collections` 提供了多个`synchronizedXxx()`方法·，该方法可以将指定集合包装成线程同步的集合，从而解决多线程并发访问集合时的线程安全问题。

我们知道 `HashSet`，`TreeSet`，`ArrayList`,`LinkedList`,`HashMap`,`TreeMap` 都是线程不安全的。`Collections` 提供了多个静态方法可以把他们包装成线程同步的集合。

**最好不要用下面这些方法，效率非常低，需要线程安全的集合类型时请考虑使用 JUC 包下的并发集合。**

方法如下：

```
synchronizedCollection(Collection<T>  c) //返回指定 collection 支持的同步（线程安全的）collection。
synchronizedList(List<T> list)//返回指定列表支持的同步（线程安全的）List。
synchronizedMap(Map<K,V> m) //返回由指定映射支持的同步（线程安全的）Map。
synchronizedSet(Set<T> s) //返回指定 set 支持的同步（线程安全的）set。
```

