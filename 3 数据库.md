[MySQL数据库面试题（2020最新版）](https://blog.csdn.net/ThinkWon/article/details/104778621?ops_request_misc=%7B%22request%5Fid%22%3A%22161686097116780274121957%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&request_id=161686097116780274121957&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~hot_rank-1-104778621.first_rank_v2_pc_rank_v29&utm_term=mysql最左前缀原则)

# 前言

首先说一下我只给问题不给答案的原因，虽然这些问题被通称为八股文，但是根据个人知识面的广度和深度每个人都会给出不一样的答案。比如常见问题进程和线程的区别，大多数人都会答到进程是资源分配的基本单位，分配的资源中很重要的便是内存空间。了解更深的同学可能知道内存空间被划分为了代码段、数据段、BSS段等，也有同学可能没有了解过什么是分段。所以答案因人而异，强行背别人的答案是没有意义的，最好能在自己的知识体系内答好一道题，避免提到了一些概念却经不住面试官的连环问。

其次我觉得如果大家时间还充足（我认为从现在开始准备到秋招应该是足够的），把这些问题当做查漏补缺比较好，如果只准备这些问题，也很容易露馅。最好的就是系统性的学习以后，把它们当做重点，让自己的知识体系能够围绕这些点展开并展示给面试官，否则只有孤零零的几个问题的答案，还是很容易被面试官问倒的。当然，如果时间紧急可以对着这些问题背一背，应该也足够应付大部分面试中遇到的问题了。

最后，大家当做参考就可以，因为我看到某个问题想到的知识点和大家想到的可能不太一样，所以我们之间难免有信息丢失。比如提到mvcc我会想到当前读和快照读、底层实现是每行的两个隐藏列+undo log、在读已提交和可重复读两个隔离级别下mvcc也会有不同的表现、具体如何利用mvcc解决脏读和不可重复读等等知识，所以建议大家找靠谱的书然后系统性的学习，这样才能和面试官谈笑风生。MySQL这里推荐《MySQL技术内幕——InnoDB存储引擎》

老样子，还是三层。

## 极高频，不会很危险

知道哪些存储引擎，InnoDB和MyISAM区别和特点，适用于什么场景

索引使用的数据结构，为什么使用b+树，和b树、红黑树、hash表的比较

什么是聚集索引、辅助索引，如何避免回表，覆盖索引

联合索引及其要求（最左匹配等，通常考察方式是给一个表和sql问是否走索引及为什么）

事务的ACID四大特性、事务的隔离级别、脏读、不可重复读、幻读

## 高频，应当熟练

多表查询中内连接、外连接、笛卡尔积等

select * from t where * group by * having * order by * limit * 标准查询语句（字节常考SQL题）

三大范式

一条SQL语句查询很慢有哪些解决方法

SQL注入攻击及预编译

你知道MySQL中有哪些日志（重点是binary log、redo log、undo log），他们的区别、作用是什么？

MVCC的作用、原理、MySQL是怎么实现的

索引优化，选取怎样的列作索引列，怎么安排索引列的顺序，如果有几个查询任务你怎样建立索引，索引失效的场景有哪些

读锁、写锁、（后面几个是我觉得完整的知识体系必须具备的，面试中能主动提到或者答上来绝对是极大的加分点：意向锁、next-key lock、gap lock）

MySQL两个账户相互转账会发生什么，如何解决出现的死锁问题

MySQL是如何解决脏读、不可重复读、幻读的？前两个是mvcc、后一个是next-key lock

事务的持久性是如何保证的？（可以想想隔离性、原子性、一致性又是如何保证的，这三个不常问，但是勤思考总是没错的）

## 一般，拓宽知识，加分项

MySQL支持的基本数据类型（问的较少，但是不会的话场面会很难看）

主键、外键、唯一约束、视图等知识

分区分表（根据功能、业务、访问频率等按列分，根据时间、用户、hash等按行分）

查询语句优化技巧，如覆盖索引的延迟关联、limit 10000000， 100语句的优化、explain的运用

一条sql语句的执行过程（包括查询和修改两大类操作）

什么是快照读和当前读

MySQL复制是如何实现的？主从复制的好处是什么



# MySQL

## 基础语法

聚合函数select count(*) from user

## 数据库设计三范式

[数据库设计三大范式](https://blog.csdn.net/qq30211478/article/details/78040519?ops_request_misc=%7B%22request%5Fid%22%3A%22161780372116780357231014%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=161780372116780357231014&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-78040519.first_rank_v2_pc_rank_v29&utm_term=数据库设计的三范式&spm=1018.2226.3001.4187)

1.第一范式（1NF）：确保每列保持原子性即列不可分

2.第二范式（2NF）：属性完全依赖于主键（属性都是该对象拥有的）

第二范式在第一范式的基础上更进一层，第二范式需要确保数据库表中每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。

利用对象之间的关系设计表，确认表的个数

3.第三范式（3NF）:属性和主键不能间接相关（减少数据冗余，这样就可以通过主外键进行表之间连接）

第三范式：数据不能存在传递关系，**即每个属性都跟主键有直接关系而不是间接关系。**

像：a-->b-->c  属性之间含有这样的关系，是不符合第三范式的。第三范式是对第二范式的细化。

## [什么是MySQL?](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=什么是mysql)

MySQL 是一种关系型数据库，在Java企业级开发中非常常用，因为 MySQL 是开源免费的，并且方便扩展。阿里巴巴数据库系统也大量用到了 MySQL，因此它的稳定性是有保障的。MySQL是开放源代码的，因此任何人都可以在 GPL(General Public License) 的许可下下载并根据个性化的需要对其进行修改。MySQL的默认端口号是**3306**。

## [存储引擎](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=存储引擎)

MySQL 当前默认的存储引擎是InnoDB,并且在5.7版本所有的存储引擎中只有 InnoDB 是事务性存储引擎，也就是说只有 InnoDB 支持事务。

![查看MySQL提供的所有存储引擎](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\mysql-engines.png)

### **InnoDB存储引擎**

**InnoDB**是事务型数据库的首选引擎，**支持事务安全表（ACID），支持行锁定和外键**。**MySQL5.5.5**之后，InnoDB 作为默认的存储引擎，文件名为frm，InnoDB 主要特性有：

- 支持事务
- 灾难恢复性好
- 为处理巨大数据量的最大性能设计
- 实现了**缓冲管理**，不仅能**缓冲索引也能缓冲数据**，并且会**自动创建散列索引以加快数据的获取**
- 支持**外键完整性约束**。存储表中的数据时，每张表的存储都按逐渐顺序存放，如果没有显示在表定义时指定主键，InnoDB会为每一行生成一个6B的ROWID,并以此作为主键。
- 被用在众多需要高性能的大型数据库站点上

### **MyISAM存储引擎**

**MyISAM** 基于 ISAM 的存储引擎，并对其进行扩展。它是在Web、数据存储和其他应用环境下最常使用的存储引擎之一。**MyISAM 拥有较高的插入、查询速度，但不支持事务**。在 **MySQL5.5.5 之前**的版本中，MyISAM 是默认的存储引擎。MyISAM 主要特性有：

- 不支持事务
- 使用**表级锁**，并发性差
- 主机宕机后，MyISAM表易损坏，灾难恢复性不佳
- 可以配合锁，实现操作系统下的复制备份、迁移
- **只缓存索引**，**数据的缓存是利用操作系统缓冲区来实现的**。可能引发过多的系统调用且效率不佳
- **数据紧凑存储**，因此可获得**更小的索引**和**更快的全表扫描性能**
- **可以把数据文件和索引文件放在不同目录**
- 用 MyISAM 引擎创建数据库，将产生3个文件。文件的名字以表的名字开始，扩展名指出文件类型：frm 文件、存储表定义，数据文件的扩展名为 .MYD（MYData），索引文件的扩展名是 .MYI（MYIndex）。

### MEMORY存储引擎

**MEMORY 存储引擎**将表中的**数据存储在内存**中，为查询和引用其他表数据提供快速访问。MEMORY 主要特性有：

- 使用**表级锁**，虽然内存访问快，但如果**频繁的读写，表级锁会成为瓶颈**
- **只支持固定大小的行**。Varchar类型的字段会存储为固定长度的Char类型，浪费空间
- 不支持TEXT、BLOB字段。当有些查询需要使用到临时表（使用的也是MEMORY存储引擎）时，如果表中有TEXT、BLOB字段，那么会转换为基于磁盘的MyISAM表，严重降低性能
- 由于内存资源成本昂贵，一般不建议设置过大的内存表，如果内存表满了，可通过清除数据或调整内存表参数来避免报错
- 服务器重启后数据会丢失，复制维护时需要小心

### **存储引擎的选择**

不同存储引擎都有各自的特点，以适应不同的需求，如下表所示，为了做出选择，首先需要考虑每一个存储引擎提供了哪些不同的功能。

| 功能         | InnoDB | MyISAM | Memory |
| ------------ | ------ | ------ | ------ |
| 存储限制     | 64TB   | 256TB  | RAM    |
| 支持事务     | Yes    | No     | No     |
| 支持全文索引 | No     | Yes    | No     |
| 支持树索引   | Yes    | Yes    | Yes    |
| 支持哈希索引 | No     | No     | Yes    |
| 支持数据缓存 | Yes    | No     | N/A    |
| 支持外键     | Yes    | No     | No     |

- 如果要**提供提交、回滚和崩溃恢复能力的事务安全（ACID兼容）能力**，并要求实现**并发控制**，**InnoDB** 是个很好的选择。
- 如果数据表主要用来**插入和查询记录**，则 **MyISAM 引擎**能提供较高的处理效率。
- 如果只是**临时存放数据，数据量不大，并且不需要较高的数据安全性**，可以将数据保存在内存中的 **Memory 引擎**。MySQL 中使用该引擎作为临时表，存放查询的中间结果。
- 如果只有 **INSERT 和 SELECT 操作，可以选择 Archive 引擎，Archive 存储引擎支持高并发的插入操作**，但是本身**并不是事务安全**的。Archive 引擎非常适合**存储归档数据**，如**记录日志信息可以使用 Archive 引擎**。

### [MyISAM和InnoDB区别](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=myisam和innodb区别)

**MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引**        

**InnoDB表文件含义：.frm表定义，表空间数据和日志文件**

MyISAM是MySQL的默认数据库引擎（5.5版之前）。虽然性能极佳，而且提供了大量的特性，包括全文索引、压缩、空间函数等，但MyISAM不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复。不过，5.5版本之后，MySQL引入了InnoDB（事务性数据库引擎），MySQL 5.5版本后默认的存储引擎为InnoDB。

大多数时候我们使用的都是 InnoDB 存储引擎，但是在某些情况下使用 MyISAM 也是合适的比如读密集的情况下。（如果你不介意 MyISAM 崩溃恢复问题的话）。

**两者的对比：**

1. **是否支持行级锁** : MyISAM 只有表级锁(table-level locking)，而InnoDB 支持行级锁(row-level locking)和表级锁,默认为**行级锁**。
2. **是否支持事务和崩溃后的安全恢复： MyISAM** 强调的是性能，每次查询具有原子性,其执行速度比InnoDB类型更快，但是不提供事务支持。但是**InnoDB** 提供事务支持，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。
3. **是否支持外键：** MyISAM不支持，而InnoDB支持。
4. **是否支持MVCC** ：仅 InnoDB 支持。应对高并发事务, MVCC比单纯的加锁更高效;MVCC只在 `READ COMMITTED` 和 `REPEATABLE READ` 两个隔离级别下工作;MVCC可以使用 **乐观(optimistic)锁** 和 **悲观(pessimistic)锁**来实现;各数据库中MVCC实现并不统一。

> **多版本并发控制**(MVCC: Multi-version concurrency control): 指的是一种提高并发的技术。最早的数据库系统，只有读读之间可以并发，读写，写读，写写都要阻塞。引入多版本之后，只有写写之间相互阻塞，其他三种操作都可以并行，这样大幅度提高了InnoDB的并发度。在内部实现中，与Postgres在数据行上实现多版本不同，InnoDB是在undolog中实现的，通过undolog可以找回数据的历史版本。找回的数据历史版本可以提供给用户读(按照隔离级别的定义，有些读请求只能看到比较老的数据版本)，也可以在回滚的时候覆盖数据页上的数据。在InnoDB内部中，会记录一个全局的活跃读写事务数组，其主要用来判断事务的可见性。

> 不要轻易相信“MyISAM比InnoDB快”之类的经验之谈，这个结论往往不是绝对的。在很多我们已知场景中，InnoDB的速度都可以让MyISAM望尘莫及，尤其是用到了聚簇索引，或者需要访问的数据都可以放入内存的应用。

一般情况下我们选择 InnoDB 都是没有问题的，但是某些情况下你并不在乎可扩展能力和并发能力，也不需要事务支持，也不在乎崩溃后的安全恢复问题的话，选择MyISAM也是一个不错的选择。但是一般情况下，我们都是需要考虑到这些问题的。

### [字符集及校对规则](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=字符集及校对规则)

字符集指的是一种从二进制编码到某类字符符号的映射。校对规则则是指某种字符集下的排序规则。MySQL中每一种字符集都会对应一系列的校对规则。

MySQL采用的是类似继承的方式指定字符集的默认值，每个数据库以及每张数据表都有自己的默认值，他们逐层继承。比如：某个库中所有表的默认字符集将是该数据库所指定的字符集（这些表在没有指定字符集的情况下，才会采用默认字符集）

## [索引](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=索引)

### [索引类型](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=索引类型)

#### [主键索引(Primary Key)](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=主键索引primary-key)

**数据表的主键列使用的就是主键索引。**

**一张数据表有只能有一个主键，并且主键不能为 null，不能重复。**

**在 mysql 的 InnoDB 的表中，当没有显示的指定表的主键时，InnoDB 会自动先检查表中是否有唯一索引的字段，如果有，则选择该字段为默认的主键，否则 InnoDB 将会自动创建一个 6Byte 的自增主键。**

#### [二级索引(辅助索引)](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=二级索引辅助索引)

**二级索引又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置。**

唯一索引，普通索引，前缀索引等索引属于二级索引。

**PS:不懂的同学可以暂存疑，慢慢往下看，后面会有答案的，也可以自行搜索。**

1. **唯一索引(Unique Key)** ：唯一索引也是一种约束。**唯一索引的属性列不能出现重复的数据，但是允许数据为 NULL，一张表允许创建多个唯一索引。** 建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。

   MySQL会在有新记录插入数据表时，自动检查新记录的这个字段的值是否已经在某个记录的这个字段里出现过了；如果是，MySQL将拒绝插入那条新记录。也就是说，**唯一索引可以保证数据记录的唯一性**。

2. **普通索引(Index)** ：**普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和 NULL。**

3. **前缀索引(Prefix)** ：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。

4. **全文索引(Full Text)** ：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6 之前只有 MYISAM 引擎支持全文索引，5.6 之后 InnoDB 也支持了全文索引。

### [为什么要用索引?索引的优缺点分析](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=为什么要用索引索引的优缺点分析)

**索引的优点**

**可以大大加快 数据的检索速度（大大减少的检索的数据量）, 这也是创建索引的最主要的原因。毕竟大部分系统的读请求总是大于写请求的。** 另外，通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。

**索引的缺点**

1. **创建索引和维护索引需要耗费许多时间**：当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执行效率。

2. **占用物理存储空间** ：索引需要使用物理文件存储，也会耗费一定空间。

   

MySQL索引使用的数据结构主要有**BTree索引** 和 **哈希索引** 。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为**单条记录查询**的时候，可以选择**哈希索引**，查询性能最快；**其余大部分场景，建议选择BTree索引**。

MySQL的BTree索引使用的是**B树中的B+Tree**，但对于主要的两种存储引擎的实现方式是不同的。

- **MyISAM:** B+Tree叶节点的**data域存放的是数据记录的地址**。**在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录**。这被称为“非聚簇索引”。
- **InnoDB:** 其数据文件本身就是索引文件。相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+Tree组织的一个索引结构，**树的叶节点data域保存了完整的数据记录**。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。这被称为“聚簇索引（或聚集索引）”。而其余的索引都作为辅助索引，辅助索引的data域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方。**在根据主索引搜索时，直接找到key所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引。** **因此，在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。**

### **B树**

因为内存的易失性。一般情况下，我们都会选择将user表中的数据和索引存储在磁盘这种外围设备中。

但是和内存相比，从磁盘中读取数据的速度会慢上百倍千倍甚至万倍，所以，我们应当尽量**减少从磁盘中读取数据的次数。** **另外，从磁盘中读取数据时，都是按照磁盘块来读取的，并不是一条一条的读。** 

如果我们能把尽量多的数据放进磁盘块中，那一次磁盘读取操作就会读取更多数据，那我们查找数据的时间也会大幅度降低。 

如果我们用树这种数据结构作为索引的数据结构，那我们每查找一次数据就需要从磁盘中读取一个节点，也就是我们说的一个磁盘块，我们都知道平衡二叉树可是每个节点只存储一个键值和数据的。

那说明什么？

说明每个磁盘块仅仅存储一个键值和数据！

那如果我们要存储海量的数据呢？

可以想象到二叉树的节点将会非常多，高度也会及其高，我们查找数据时也会进行很多次磁盘IO，我们查找数据的效率将会极低！

![image-20210324201613905](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\image-20210324201613905.png)

为了解决平衡二叉树的这个弊端，我们应该寻找一种单个节点可以存储多个键值和数据的平衡树。也就是我们接下来要说的B树。 

B树（Balance Tree）即为平衡树的意思，下图即是一颗B树。

![image-20210324201625138](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\image-20210324201625138.png)

图中的p节点为指向子节点的指针，二叉查找树和平衡二叉树其实也有，因为图的美观性，被省略了。- 图中的每个节点称为页，页就是我们上面说的磁盘块，在mysql中数据读取的基本单位都是页，所以我们这里叫做页更符合mysql中索引的底层数据结构。

从上图可以看出，**B树相对于平衡二叉树，每个节点存储了更多的键值(key)和数据(data)，并且每个节点拥有更多的子节点，子节点的个数一般称为阶，上述图中的B树为3阶B树，高度也会很低。** 

基于这个特性，B树查找数据读取磁盘的次数将会很少，数据的查找效率也会比平衡二叉树高很多。 

假如我们要查找id=28的用户信息，那么我们在上图B树中查找的流程如下： 

- 先找到根节点也就是页1，判断28在键值17和35之间，我们那么我们根据页1中的指针p2找到页3。 
- 将28和页3中的键值相比较，28在26和30之间，我们根据页3中的指针p2找到页8。 
- 将28和页8中的键值相比较，发现有匹配的键值28，键值28对应的用户信息为(28,bv)。

### **B+树**

B+树是对B树的进一步优化。让我们先来看下B+树的结构图：

![image-20210324201717320](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\image-20210324201717320.png)

￼根据上图我们来看下B+树和B树有什么不同。 

1. B+树非叶子节点上是不存储数据的，仅存储键值，而B树节点中不仅存储键值，也会存储数据。之所以这么做是因为在数据库中页的大小是固定的，innodb中页的默认大小是16KB。如果不存储数据，那么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就会更矮更胖，如此一来我们查找数据进行磁盘的IO次数有会再次减少，数据查询的效率也会更快。

另外，B+树的阶数是等于键值的数量的，如果我们的B+树一个节点可以存储1000个键值，那么3层B+树可以存储1000×1000×1000=10亿个数据。一般根节点是常驻内存的，所以一般我们查找10亿数据，只需要2次磁盘IO。 

2. 因为B+树索引的所有数据均存储在叶子节点，而且数据是按照顺序排列的。那么B+树使得范围查找，排序查找，分组查找以及去重查找变得异常简单。而B树因为数据分散在各个节点，要实现这一点是很不容易的。  

有心的读者可能还发现上图B+树中各个页之间是通过双向链表连接的，叶子节点中的数据是通过单向链表连接的。

其实上面的B树我们也可以对各个节点加上链表。其实这些不是它们之前的区别，是因为在mysql的innodb存储引擎中，索引就是这样存储的。也就是**说上图中的B+树索引就是innodb中B+树索引真正的实现方式，准确的说应该是聚集索引（聚集索引和非聚集索引下面会讲到）。**

通过上图可以看到，在innodb中，我们通过数据页之间通过双向链表连接以及叶子节点中数据之间通过单向链表连接的方式可以找到表中所有的数据。

MyISAM中的B+树索引实现与innodb中的略有不同。在MyISAM中，B+树索引的叶子节点并不存储数据，而是存储数据的文件地址。

### [B 树和 B+树区别](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=b-树和-b树区别)

- B 树的所有节点既存放键(key) 也存放 数据(data);而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key。
- B 树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。
- B 树的检索的过程相当于对范围内的每个节点的关键字做**二分查找**，可能还没有到达叶子节点，检索就结束了。而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的**顺序检索**很明显。

### [Hash 索引和 B+树索引优劣分析](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=hash-索引和-b树索引优劣分析)

**Hash 索引定位快**

Hash 索引指的就是 Hash 表，最大的优点就是**能够在很短的时间内，根据 Hash 函数定位到数据所在的位置**，这是 B+树所不能比的。

**Hash 冲突问题**

知道 HashMap 或 HashTable 的同学，相信都知道它们最大的缺点就是 Hash 冲突了。不过对于数据库来说这还不算最大的缺点。

**Hash 索引不支持顺序和范围查询是它最大的缺点。**

试想一种情况:

```text
SELECT * FROM tb1 WHERE id < 500;
```

B+树是有序的，在这种范围查询中，优势非常大，直接遍历比 500 小的叶子节点就够了。而 Hash 索引是根据 hash 算法来定位的，难不成还要把 1 - 499 的数据，每个都进行一次 hash 计算来定位吗?这就是 Hash 最大的缺点了。

### [聚集索引与非聚集索引](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=聚集索引与非聚集索引)

**[聚集索引](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=聚集索引)**

**聚集索引即索引结构和数据一起存放的索引。主键索引属于聚集索引。**

在 Mysql 中，InnoDB 引擎的表的 `.ibd`文件就包含了该表的索引和数据，对于 InnoDB 引擎表来说，该表的索引(B+树)的每个非叶子节点存储索引，叶子节点存储索引和索引对应的数据。

**[聚集索引的优点](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=聚集索引的优点)**

聚集索引的**查询速度非常的快**，因为整个 B+树本身就是一颗多叉平衡树，**叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。**

**[聚集索引的缺点](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=聚集索引的缺点)**

1. **依赖于有序的数据** ：因为 B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或 UUID 这种又长又难比较的数据，插入或查找的速度肯定比较慢。
2. **更新代价大** ： 如果对索引列的数据被修改时，那么对应的索引也将会被修改， 而且况聚集索引的叶子节点还存放着数据，修改代价肯定是较大的， 所以对于主键索引来说，主键一般都是不可被修改的。

**[非聚集索引](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=非聚集索引)**

**非聚集索引即索引结构和数据分开存放的索引。**

**二级索引属于非聚集索引。**

> MYISAM 引擎的表的.MYI 文件包含了表的索引， 该表的索引(B+树)的每个叶子非叶子节点存储索引， 叶子节点存储索引和索引对应数据的指针，指向.MYD 文件的数据。
>
> **非聚集索引的叶子节点并不一定存放数据的指针， 因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。**

**[非聚集索引的优点](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=非聚集索引的优点)**

**更新代价比聚集索引要小** 。非聚集索引的更新代价就没有聚集索引那么大了，非聚集索引的叶子节点是不存放数据的

**[非聚集索引的缺点](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=非聚集索引的缺点)**

1. 跟聚集索引一样，非聚集索引也依赖于有序的数据
2. **可能会二次查询(回表)** :这应该是非聚集索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。（如果建立了带查询字段的索引就不需要回表了，覆盖索引）

### [索引创建原则](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=索引创建原则)

- **[单列索引](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=单列索引)**：单列索引即由一列属性组成的索引。

- **[联合索引(多列索引)](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=联合索引多列索引)**：联合索引即由多列属性组成索引。

- **[最左前缀原则](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=最左前缀原则)**假设创建的联合索引由三个字段组成:

```text
ALTER TABLE table ADD INDEX index_name (num,name,age)Copy to clipboardErrorCopied
```

那么当查询的条件有为:num / (num AND name) / (num AND name AND age)时，索引才生效。所以在创建联合索引时，尽量把查询最频繁的那个字段作为最左(第一个)字段。查询的时候也尽量以这个字段为第一条件。从最左边开始匹配

**[选择合适的字段](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=选择合适的字段)**

**[1.不为 NULL 的字段](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=_1不为-null-的字段)**

索引字段的数据应该尽量不为 NULL，因为对于数据为 NULL 的字段，数据库较难优化。如果字段频繁被查询，但又避免不了为 NULL，建议使用 0,1,true,false 这样语义较为清晰的短值或短字符作为替代。

**[2.被频繁查询的字段](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=_2被频繁查询的字段)**

我们创建索引的字段应该是查询操作非常频繁的字段。

**[3.被作为条件查询的字段](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=_3被作为条件查询的字段)**

被作为 WHERE 条件查询的字段，应该被考虑建立索引。

**[4.被经常频繁用于连接的字段](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=_4被经常频繁用于连接的字段)**

经常用于连接的字段可能是一些外键列，对于外键列并不一定要建立外键，只是说该列涉及到表与表的关系。对于频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率。

**[不合适创建索引的字段](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=不合适创建索引的字段)**

**[1.被频繁更新的字段应该慎重建立索引](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=_1被频繁更新的字段应该慎重建立索引)**

虽然索引能带来查询上的效率，但是维护索引的成本也是不小的。 如果一个字段不被经常查询，反而被经常修改，那么就更不应该在这种字段上建立索引了。

**[2.不被经常查询的字段没有必要建立索引](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=_2不被经常查询的字段没有必要建立索引)**

**[3.尽可能的考虑建立联合索引而不是单列索引](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=_3尽可能的考虑建立联合索引而不是单列索引)**

因为索引是需要占用磁盘空间的，可以简单理解为每个索引都对应着一颗 B+树。如果一个表的字段过多，索引过多，那么当这个表的数据达到一个体量后，索引占用的空间也是很多的，且修改索引时，耗费的时间也是较多的。如果是联合索引，多个字段在一个索引上，那么将会节约很大磁盘空间，且修改数据的操作效率也会提升。

**[4.注意避免冗余索引](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=_4注意避免冗余索引)**

冗余索引指的是索引的功能相同，能够命中索引(a, b)就肯定能命中索引(a) ，那么索引(a)就是冗余索引。如（name,city ）和（name ）这两个索引就是冗余索引，能够命中前者的查询肯定是能够命中后者的 在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。

**[5.考虑在字符串类型的字段上使用前缀索引代替普通索引](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=_5考虑在字符串类型的字段上使用前缀索引代替普通索引)**

前缀索引仅限于字符串类型，较普通索引会占用更小的空间，所以可以考虑使用前缀索引带替普通索引。

### [使用索引一定能提高查询性能吗?](https://snailclimb.gitee.io/javaguide/#/docs/database/数据库索引?id=使用索引一定能提高查询性能吗)

大多数情况下，索引查询都是比全表扫描要快的。但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升。

## [什么是事务?](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=什么是事务)

**事务是逻辑上的一组操作，要么都执行，要么都不执行。**

事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。

### [事务的四大特性(ACID)](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=事务的四大特性acid)

![事务的特性](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\事务特性.png)

1. **原子性（Atomicity）：** 事务是最小的执行单位，不允许分割。事务的原子性确保**动作要么全部完成，要么完全不起作用**；
2. **一致性（Consistency）：** **执行事务后，数据库从一个正确的状态变化到另一个正确的状态**；
3. **隔离性（Isolation）：** **并发访问**数据库时，**一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的**；
4. **持久性（Durability）：** **一个事务被提交之后，它对数据库中数据的改变是持久的**，即使数据库发生故障也不应该对其有任何影响。

**事务的常用语句**

- `START TARNSACTION` |`BEGIN`：显式地开启一个事务。
- `COMMIT`：提交事务，使得对数据库做的所有修改成为永久性。
- `ROLLBACK`：回滚会结束用户的事务，并撤销正在进行的所有未提交的修改。

### [并发事务带来哪些问题?](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=并发事务带来哪些问题)

在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。

- **脏读（Dirty read）:** 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。
- **丢失修改（Lost to modify）:** 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。
- **不可重复读（Unrepeatableread）:** 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。
- **幻读（Phantom read）:** 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。

**不可重复读和幻读区别：**

不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。

### [事务隔离级别有哪些?MySQL的默认隔离级别是?](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=事务隔离级别有哪些mysql的默认隔离级别是)

**SQL 标准定义了四个隔离级别：**

- **READ-UNCOMMITTED(读取未提交)：** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**。
- **READ-COMMITTED(读取已提交)：** 允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**。
- **REPEATABLE-READ(可重复读)：** 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生**。
- **SERIALIZABLE(可串行化)：** 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。

------

| 隔离级别         | 脏读 | 不可重复读 | 幻读 |
| ---------------- | ---- | ---------- | ---- |
| READ-UNCOMMITTED | √    | √          | √    |
| READ-COMMITTED   | ×    | √          | √    |
| REPEATABLE-READ  | ×    | ×          | √    |
| SERIALIZABLE     | ×    | ×          | ×    |

MySQL InnoDB 存储引擎的默认支持的隔离级别是 **REPEATABLE-READ（可重读）**。

这里需要注意的是：与 SQL 标准不同的地方在于 InnoDB 存储引擎在 **REPEATABLE-READ（可重读）** 事务隔离级别下使用的是Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server) 是不同的。所以说InnoDB 存储引擎的默认支持的隔离级别是 **REPEATABLE-READ（可重读）** 已经可以完全保证事务的隔离性要求，即达到了 SQL标准的 **SERIALIZABLE(可串行化)** 隔离级别。因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 **READ-COMMITTED(读取提交内容)** ，但是你要知道的是InnoDB 存储引擎默认使用 **REPEATABLE-READ（可重读）** 并不会有任何性能损失，即可以满足事务的隔离性要求，采用next-key lock锁算法。

InnoDB 存储引擎在 **分布式事务** 的情况下一般会用到 **SERIALIZABLE(可串行化)** 隔离级别。

### [锁机制与InnoDB锁算法（解决并发和数据安全的问题）](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=锁机制与innodb锁算法)

**MyISAM和InnoDB存储引擎使用的锁：**

- MyISAM采用表级锁(table-level locking)。
- InnoDB支持行级锁(row-level locking)和表级锁,默认为行级锁

**表级锁和行级锁对比：**

- **表级锁：** MySQL中锁定 **粒度最大** 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。

- **行级锁：** MySQL中锁定 **粒度最小** 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。

  **InnoDB存储引擎的锁的算法有三种：**

  - Record lock：单个行记录上的锁，其他事务不能修改和删除加锁项
  - Gap lock：间隙锁，锁定一个范围，不包括记录本身，其他事务不能在锁范围内插入数据，这样就防止了别的事务新增幻影行，解决幻读。
  - Next-key lock：record+gap 锁定一个范围，包含记录本身，可解决幻读问题。

**相关知识点：**

1. innodb对于行的查询使用next-key lock
2. next-key lock为了解决Phantom Problem幻读问题
3. 当查询的索引含有唯一属性时，将next-key lock降级为record key
4. Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生
5. 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1

**死锁和避免死锁**

**InnoDB的行级锁是基于索引实现的**，如果查询语句为**命中任何索引**，那么**InnoDB会使用表级锁**. 此外，InnoDB的**行级锁是针对索引加的锁，不针对数据记录**，因此即使访问不同行的记录，如果使用了相同的索引键仍然会出现锁冲突，还需要注意的是，在通过

```
SELECT ...LOCK IN SHARE MODE;
```

或

```
SELECT ...FOR UPDATE;
```

使用锁的时候，如果表没有定义任何索引，那么InnoDB会创建一个隐藏的聚簇索引并使用这个索引来加记录锁。

此外，不同于**MyISAM总是一次性获得所需的全部锁**，**InnoDB的锁是逐步获得的**，当两个事务都需要获得对方持有的锁，导致双方都在等待，这就产生了死锁。 发生死锁后，InnoDB一般都可以检测到，并**使一个事务释放锁回退**，另一个则可以获取锁完成事务，我们可以采取以上方式避免死锁：

- 通过**表级锁**来减少死锁产生的概率；
- 多个程序尽量约定以相同的顺序访问表（这也是解决并发理论中哲学家就餐问题的一种思路）；
- 同一个事务尽可能做到一次锁定所需要的所有资源。

### **数据库的乐观锁和悲观锁是什么？怎么实现的？**

数据库管理系统（DBMS）中的并发控制的任务是**确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性**。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。

**悲观锁**：假定**会发生并发冲突，屏蔽一切可能违反数据完整性的操作**。在**查询完数据的时候就把事务锁起来**，直到提交事务。

实现方式：**使用数据库中的锁机制**

**乐观锁**：假设**不会发生并发冲突，只在提交操作时检查是否违反数据完整性**。在**修改数据的时候把事务锁起来**，通过version版本号的方式来进行锁定。

实现方式：一般会使用**版本号机制**或**CAS算法**实现

**两种锁的使用场景**

从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像**乐观锁适用于写比较少的情况下（多读场景）**，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。

但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般**多写的场景下用悲观锁就比较合适。**



## MySQL日志

日志是mysql数据库的重要组成部分，记录着数据库运行期间各种状态信息。mysql日志主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。

作为开发，我们重点需要关注的是**二进制日志(binlog)**和**事务日志(包括redo log和undo log)**

### binlog

binlog用于记录数据库执行的写入性操作(不包括查询)信息，以二进制的形式保存在磁盘中。binlog是mysql的逻辑日志，并且由Server层进行记录，使用任何存储引擎的mysql数据库都会记录binlog日志。

- 逻辑日志：可以简单理解为记录的就是sql语句。
- 物理日志：因为mysql数据最终是保存在数据页中的，物理日志记录的就是数据页变更。

binlog是通过追加的方式进行写入的，可以通过max_binlog_size参数设置每个binlog文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志。

**binlog使用场景**

在实际应用中，binlog的主要使用场景有两个，分别是主从复制和数据恢复。

- 主从复制：在Master端开启binlog，然后将binlog发送到各个Slave端，Slave端重放binlog从而达到主从数据一致。
- 数据恢复：通过使用mysqlbinlog工具来恢复数据。

**binlog刷盘时机**

对于InnoDB存储引擎而言，只有在事务提交时才会记录binlog，此时记录还在内存中，那么binlog是什么时候刷到磁盘中的呢？mysql通过sync_binlog参数控制biglog的刷盘时机，取值范围是0-N：

- 0：不去强制要求，由系统自行判断何时写入磁盘；
- 1：每次commit的时候都要将binlog写入磁盘；
- N：每N个事务，才会将binlog写入磁盘。

从上面可以看出，sync_binlog最安全的是设置是1，这也是MySQL 5.7.7之后版本的默认值。但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。

**binlog日志格式**

binlog日志有三种格式，分别为STATMENT、ROW和MIXED。

> 在 MySQL 5.7.7之前，默认的格式是STATEMENT，MySQL 5.7.7之后，默认值是ROW。日志格式通过binlog-format指定。

**STATMENT**

基于SQL语句的复制(statement-based replication, SBR)，每一条会修改数据的sql语句会记录到binlog中。

- 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO, 从而提高了性能；
- 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate()、sleep()等。

**ROW**

基于行的复制(row-based replication, RBR)，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了。

- 优点：不会出现某些特定情况下的存储过程、或function、或trigger的调用和触发无法被正确复制的问题；
- 缺点：会产生大量的日志，尤其是alter table的时候会让日志暴涨

**MIXED**

基于STATMENT和ROW两种模式的混合复制(mixed-based replication, MBR)，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog

### redo log

**为什么需要redo log**

我们都知道，事务的四大特性里面有一个是持久性，具体来说就是只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态。那么mysql是如何保证一致性的呢？最简单的做法是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。但是这么做会有严重的性能问题，主要体现在两个方面：

- 因为Innodb是以页为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！
- 一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差！

因此mysql设计了redo log，具体来说就是只记录事务对数据页做了哪些修改，这样就能完美地解决性能问题了(相对而言文件更小并且是顺序IO)。

**redo log基本概念**

redo log包括两部分：一个是内存中的日志缓冲(redo log buffer)，另一个是磁盘上的日志文件(redo log file)。mysql每执行一条DML语句，先将记录写入redo log buffer，后续某个时间点再一次性将多个操作记录写到redo log file。这种先写日志，再写磁盘的技术就是MySQL里经常说到的WAL(Write-Ahead Logging) 技术。

在计算机操作系统中，用户空间(user space)下的缓冲区数据一般情况下是无法直接写入磁盘的，中间必须经过操作系统内核空间(kernel space)缓冲区(OS Buffer)。因此，redo log buffer写入redo log file实际上是先写入OS Buffer，然后再通过系统调用fsync()将其刷到redo log file中，过程如下：

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\b633136f-d2e4-4ab8-b205-0c4a0efed862.png)

mysql支持三种将redo log buffer写入redo log file的时机，可以通过innodb_flush_log_at_trx_commit参数配置，各参数值含义如下：

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\815a1173-4b20-48f5-bc6e-c7bb81cfdff0.png)

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\fe051aba-3af0-4b0e-8040-e4f57777dd9a.png)

**redo log记录形式**

前面说过，redo log实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此redo log实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。如下图：

<img src="C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\53da5155-bc53-4dab-9a4f-1e82bc5252b7.png" alt="img" style="zoom:50%;" />

同时我们很容易得知，在innodb中，既有redo log需要刷盘，还有数据页也需要刷盘，redo log存在的意义主要就是降低对数据页刷盘的要求。在上图中，write pos表示redo log当前记录的LSN(逻辑序列号)位置，check point表示数据页更改记录刷盘后对应redo log所处的LSN(逻辑序列号)位置。

write pos到check point之间的部分是redo log空着的部分，用于记录新的记录；check point到write pos之间是redo log待落盘的数据页更改记录。当write pos追上check point时，会先推动check point向前移动，空出位置再记录新的日志。

启动innodb的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。因为redo log记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志(如binlog)要快很多。

重启innodb时，首先会检查磁盘中数据页的LSN，如果数据页的LSN小于日志中的LSN，则会从checkpoint开始恢复。

还有一种情况，在宕机前正处于checkpoint的刷盘过程，且数据页的刷盘进度超过了日志页的刷盘进度，此时会出现数据页中记录的LSN大于日志中的LSN，这时超出日志进度的部分将不会重做，因为这本身就表示已经做过的事情，无需再重做。

### **redo log与binlog区别**

由binlog和redo log的区别可知：binlog日志只用于归档，只依靠binlog是没有crash-safe能力的。但只有redo log也不行，因为redo log是InnoDB特有的，且日志上的记录落盘后会被覆盖掉。因此需要binlog和redo log二者同时记录，才能保证当数据库发生宕机重启时，数据不会丢失。

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\bcc825f0-e2ef-4f93-abf1-83684e5f09ff.png)

### undo log

数据库事务四大特性中有一个是**原子性**，具体来说就是 原子性是指对数据库的一系列操作，要么全部成功，要么全部失败，不可能出现部分成功的情况。

实际上，原子性底层就是通过undo log实现的。undo log主要记录了数据的逻辑变化，比如一条INSERT语句，对应一条DELETE的undo log，对于每个UPDATE语句，对应一条相反的UPDATE的undo log，这样在发生错误时，就能回滚到事务之前的数据状态。

同时，undo log也是MVCC(多版本并发控制)实现的关键。

**多版本并发控制**(MVCC: Multi-version concurrency control): 指的是一种提高并发的技术。最早的数据库系统，只有读读之间可以并发，读写，写读，写写都要阻塞。引入多版本之后，只有写写之间相互阻塞，其他三种操作都可以并行，这样大幅度提高了InnoDB的并发度。在内部实现中，与Postgres在数据行上实现多版本不同，InnoDB是在undolog中实现的，通过undolog可以找回数据的历史版本。找回的数据历史版本可以提供给用户读(按照隔离级别的定义，有些读请求只能看到比较老的数据版本)，也可以在回滚的时候覆盖数据页上的数据。在InnoDB内部中，会记录一个全局的活跃读写事务数组，其主要用来判断事务的可见性。



## [大表优化](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=大表优化)

当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下：

#### [1. 限定数据的范围](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=_1-限定数据的范围)

务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内；

#### [2. 读/写分离](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=_2-读写分离)

经典的数据库拆分方案，主库负责写，从库负责读；

#### [3. 垂直分区](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=_3-垂直分区待完善)

**根据数据库里面数据表的相关性进行拆分。** 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。

**简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。** 如下图所示，这样来说大家应该就更容易理解了。 ![数据库垂直分区](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\数据库垂直分区.png)

- **垂直拆分的优点：** 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。
- **垂直拆分的缺点：** 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂；

#### [4. 水平分区](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=_4-水平分区待完善)

**保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。**

水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。

![数据库水平拆分](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\数据库水平拆分.png)

水平拆分可以支持非常大的数据量。需要注意的一点是：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 **水平拆分最好分库** 。

水平拆分能够 **支持非常大的数据量存储，应用端改造也少**，但 **分片事务难以解决** ，跨节点Join性能较差，逻辑复杂。《Java工程师修炼之道》的作者推荐 **尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度** ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。

**下面补充一下数据库分片的两种常见方案：**

- **客户端代理：** **分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。** 当当网的 **Sharding-JDBC** （推荐） 、阿里的TDDL是两种比较常用的实现。
- **中间件代理：** **在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。** 我们现在谈的 **Mycat** 、360的Atlas、网易的DDB等等都是这种架构的实现。

### [解释一下什么是池化设计思想。什么是数据库连接池?为什么需要数据库连接池?](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=解释一下什么是池化设计思想。什么是数据库连接池为什么需要数据库连接池)

池化设计应该不是一个新名词。我们常见的如java线程池、jdbc连接池、redis连接池等就是这类设计的代表实现。这种设计**会初始预设资源，解决的问题就是抵消每次获取资源的消耗**，如创建线程的开销，获取远程连接的开销等。除了初始化资源，池化设计还包括如下这些特征：**池子的初始值、池子的活跃值、池子的最大值**等，这些特征可以直接映射到java线程池和数据库连接池的成员属性中。

数据库连接本质就是一个 **socket 的连接**。数据库服务端还要维护一些缓存和用户权限信息之类的 所以占用了一些内存。我们可以把**数据库连接池是看做是维护的数据库连接的缓存，以便将来需要对数据库的请求时可以重用这些连接**。为每个用户打开和维护数据库连接，尤其是对动态数据库驱动的网站应用程序的请求，既昂贵又浪费资源。**在连接池中，创建连接后，将其放置在池中，并再次使用它，因此不必建立新的连接。如果使用了所有连接，则会建立一个新连接并将其添加到池中**。 连接池还**减少了用户必须等待建立与数据库的连接的时间**。

### 分库分表之后,id 主键如何处理？snowflake

因为要是分成多个表之后，每个表都是从 1 开始累加，这样是不对的，我们需要一个**全局唯一的 id 来支持**。

生成全局 id 有下面这几种方式：

- **UUID**：不适合作为主键，因为太长了，并且无序不可读，查询效率低。比较适合用于生成唯一的名字的标示比如文件的名字。
- **数据库自增 id** : **两台数据库分别设置不同步长，生成不重复ID的策略来实现高可用。这种方式生成的 id 有序，但是需要独立部署数据库实例，成本高，还会有性能瓶颈。**
- **利用 redis 生成 id :** 性能比较好，灵活方便，不依赖于数a'a'S'a'a'a'a'a'sa'sa'q据库。但是，引入了新的组件造成系统更加复杂，可用性降低，编码更加复杂，增加了系统成本。
- **Twitter的snowflake算法** ：Github 地址：https://github.com/twitter-archive/snowflake。
- **美团的[Leaf](https://tech.meituan.com/2017/04/21/mt-leaf.html)分布式ID生成系统** ：Leaf 是美团开源的分布式ID生成器，能保证全局唯一性、趋势递增、单调递增、信息安全，里面也提到了几种分布式方案的对比，但也需要依赖关系数据库、Zookeeper等中间件。感觉还不错。美团技术团队的一篇文章：https://tech.meituan.com/2017/04/21/mt-leaf.html 。

## [外键和级联](https://snailclimb.gitee.io/javaguide/#/docs/database/阿里巴巴开发手册数据库部分的一些最佳实践?id=外键和级联)

对于外键和级联，阿里巴巴开发手册这样说到：

> 【强制】不得使用外键与级联，一切外键概念必须在应用层解决。
>
> 说明:以学生和成绩的关系为例，学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群;级联更新是强阻塞，存在数据库更新风暴的风 险;外键影响数据库的插入速度

为什么不要用外键呢？大部分人可能会这样回答：

> 1. **增加了复杂性：** a.每次做DELETE 或者UPDATE都必须考虑外键约束，会导致开发的时候很痛苦,测试数据极为不方便;b.外键的主从关系是定的，假如那天需求有变化，数据库中的这个字段根本不需要和其他表有关联的话就会增加很多麻烦。
> 2. **增加了额外工作**： 数据库需要增加维护外键的工作，比如当我们做一些涉及外键字段的增，删，更新操作之后，需要触发相关操作去检查，保证数据的的一致性和正确性，这样会不得不消耗资源；（个人觉得这个不是不用外键的原因，因为即使你不使用外键，你在应用层面也还是要保证的。所以，我觉得这个影响可以忽略不计。）
> 3. 外键还会因为需要请求对其他表内部加锁而容易出现死锁情况；
> 4. **对分库分表不友好** ：因为分库分表下外键是无法生效的。

外键也是有很多好处的，比如：

1. 保证了数据库数据的一致性和完整性；
2. 级联操作方便，减轻了程序代码量；

如果系统不涉及分不分表，并发量不是很高的情况还是可以考虑使用外键的。但最好是在应用层去进行相关操作

我个人是不太喜欢外键约束，比较喜欢在应用层去进行相关操作。

## [关于@Transactional注解](https://snailclimb.gitee.io/javaguide/#/docs/database/阿里巴巴开发手册数据库部分的一些最佳实践?id=关于transactional注解)

对于`@Transactional`事务注解，阿里巴巴开发手册这样说到：

> 【参考】@Transactional事务不要滥用。事务会影响数据库的QPS，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。

以秒杀系统为例，事务是交由mysql的存储过程去完成的，如果使用注解，会产生通信的延迟等等问题

### [Mysql中key 、primary key 、unique key 与index区别](https://www.cnblogs.com/zjfjava/p/6922494.html)

**1、如果只是key的话，就是普通索引**。

key 是数据库的**物理结构**，它包含两层意义和作用，

**一是约束（偏重于约束和规范数据库的结构完整性）**constraint

**二是索引（辅助查询用的）**key

包括primary key, unique key, foreign key 等。

####  **primary key** 

**primary key** 有两个作用，一是约束作用（constraint），用来**规范一个存储主键和唯一性**，但同时也在此key上建立了一个主键索引；  

PRIMARY KEY 约束：**唯一标识**数据库表中的每条记录；

**主键必须包含唯一的值；**

**主键列不能包含 NULL 值**；

每个表都应该有一个主键，并且每个表只能有一个主键。（PRIMARY KEY 拥有自动定义的 UNIQUE 约束）

#### **unique key** 

**unique key** 也有两个作用，一是约束作用（constraint），规范数据的**唯一性**，但同时也在这个key上建立了一个唯一索引；

UNIQUE 约束：**唯一标识**数据库表中的每条记录。 

UNIQUE 和 PRIMARY KEY 约束均为列或列集合提供了**唯一性**的保证。

每个表可以有**多个 UNIQUE 约束**，但是每个表只能有**一个 PRIMARY KEY 约束**

#### **foreign key**

**foreign key** 也有两个作用，一是约束作用（constraint），规范数据的引用完整性，但同时也在这个key上建立了一个index；

 **2、index**是数据库的物理结构，它只是辅助查询的，它创建时会在另外的表空间（mysql中innodb表空间）以一个类似目录的结构存储。索引要分类的话，分为前缀索引、全文本索引等；因此，索引只是索引，**它不会去约束索引的字段的行为**（那是key要做的事情）。

如，create table t(id int,**index inx_tx_id (id)**);

### 数据库常用语法

#### 多表查询

left join、right join、inner join

笛卡尔积，把左右两表直接相乘



![alt text](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\VQ5XP.png)

### 数据类型

| 类型      | 字节 |
| :-------- | ---- |
| bigint    | 8    |
| int       | 4    |
| mediumint | 3    |
| smallint  | 2    |
| tinyint   | 1    |

```
desc降序排列
```

```
可以用来实现分页
mysql limit offset用法实例
语句1：select * from student limit 9,4
语句2：slect * from student limit 4 offset 9
// 语句1和2均返回表student的第10、11、12、13行 
// 语句2中的4表示返回4行，9表示从表的第十行开始
```



# Redis（默认端口6379）

### [1. 简单介绍一下 Redis 呗!](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_1-简单介绍一下-redis-呗)

简单来说 **Redis 就是一个使用 C 语言开发的数据库**，不过与传统数据库不同的是 **Redis 的数据是存在内存中的** ，也就是它是内存数据库，所以读写速度非常快，因此 Redis 被广泛应用于缓存方向。

另外，Redis 除了做缓存之外，Redis 也经常用来做**分布式锁**，甚至是**消息队列**。

Redis 提供了多种数据类型来支持不同的业务场景。Redis 还**支持事务 、持久化、Lua 脚本、多种集群方案**。

### [2. 分布式缓存常见的技术选型方案有哪些？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_2-分布式缓存常见的技术选型方案有哪些？)

分布式缓存的话，使用的比较多的主要是 **Memcached** 和 **Redis**。不过，现在基本没有看过还有项目使用 **Memcached** 来做缓存，都是直接用 **Redis**。

Memcached 是分布式缓存最开始兴起的那会，比较常用的。后来，随着 Redis 的发展，大家慢慢都转而使用更加强大的 Redis 了。

**分布式缓存主要解决的是单机缓存的容量受服务器限制并且无法保存通用的信息**。因为，本地缓存只在当前服务里有效，比如如果你部署了两个相同的服务，他们两者之间的缓存数据是无法共同的。

### [3. 说一下 Redis 和 Memcached 的区别和共同点](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_3-说一下-redis-和-memcached-的区别和共同点)

**共同点** ：

1. 都是**基于内存的数据库**，一般都用来**当做缓存使用**。
2. 都有**过期策略**。
3. 两者的**性能都非常高**。

**区别** ：

1. **Redis 支持更丰富的数据类型（支持更复杂的应用场景）**。Redis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list，set，sortedset，hash 等数据结构的存储。Memcached 只支持最简单的 k/v 数据类型。k：string ；v：各种数据类型
2. **Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而 Memecache 把数据全部存在内存之中。**
3. **Redis 有灾难恢复机制。 因为可以把缓存中的数据持久化到磁盘上**。
4. **Redis 在服务器内存使用完之后，可以将不用的数据放到磁盘上。但是，Memcached 在服务器内存使用完之后，就会直接报异常。**
5. Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；**但是 Redis 目前是原生支持 cluster 模式的.**
6. Memcached 是多线程，非阻塞 IO 复用的网络模型；**Redis 使用单线程的多路 IO 复用模型**。（Redis 6.0 引入了多线程 IO ）
8. **Memcached 过期数据的删除策略只用了惰性删除**，而 **Redis 同时使用了惰性删除与定期删除**。

### [4. 缓存数据的处理流程是怎样的？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_4-缓存数据的处理流程是怎样的？)

![正常缓存处理流程](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\缓存的处理流程.png)



### [5. 为什么要用 Redis/为什么要用缓存？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_5-为什么要用-redis为什么要用缓存？)

从高性能和高并发的角度来看

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\使用缓存之后.png)

**高性能** ：

如果用户是第一次访问数据库的数据，过程比较慢，毕竟是从硬盘读取的。但如果用户访问的数据属于高频数据并且不会经常改变，可以放心地将用户访问的数据放在缓存中。

好处在于：保证用户下一次访问这些数据的时候可以直接从缓存获取，操作缓存就是操作内存，速度非常快。

难点在于：要保持数据库和缓存中数据的一致性。如果数据库中的对应数据改变之后，同步改变缓存中对应的数据即可。

**高并发：**

一般像MySQL这类的数据库的QPS大概在1w左右（4核8g），但是使用redis缓存之后很容易上10

w+，甚至可以达到30w+（单机redis，cluster会更高）

> QPS（Query Per Second）：服务器每秒可以执行的查询次数；

直接操作缓存能够承受的数据库请求数量是远远大于直接访问数据库的，所以可以考虑把数据库中的部分数据转移到缓存中，这样用户的一部分请求会直接到缓存而不用经过数据库。进而，也就提高了系统整体的并发。

### [6. Redis 常见数据结构以及使用场景分析](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_6-redis-常见数据结构以及使用场景分析)

#### [6.1. string](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_61-string)

1. **介绍** ：string 数据结构是简单的 key-value 类型。虽然 Redis 是用 C 语言写的，但是 Redis 并没有使用 C 的字符串表示，而是自己构建了一种 **简单动态字符串**（simple dynamic string，**SDS**）。相比于 C 的原生字符串，Redis 的 SDS 不光可以保存文本数据还可以保存二进制数据，并且获取字符串长度复杂度为 O(1)（C 字符串为 O(N)）,除此之外,Redis 的 SDS API 是安全的，不会造成缓冲区溢出。SDS 类似于 **Java** 中的 **ArrayList**，可以通过预分配冗余空间的方式来减少内存的频繁分配。
2. **常用命令:** `set,get,strlen,exists,dect,incr,setex` 等等。
3. **应用场景** ：一般常用在需要计数的场景，比如用户的访问次数、热点文章的点赞转发数量等等。
   1. **缓存功能：String**字符串是最常用的数据类型，不仅仅是**Redis**，各个语言都是最基本类型，因此，利用**Redis**作为缓存，配合其它数据库作为存储层，利用**Redis**支持高并发的特点，可以大大加快系统的读写速度、以及降低后端数据库的压力。
   2. **计数器：**许多系统都会使用**Redis**作为系统的实时计数器，可以快速实现计数和查询的功能。而且最终的数据结果可以按照特定的时间落地到数据库或者其它存储介质当中进行永久保存。
   3. **共享用户Session：**用户重新刷新一次界面，可能需要访问一下数据进行重新登录，或者访问页面缓存**Cookie**，但是可以利用**Redis**将用户的**Session**集中管理，在这种模式只需要保证**Redis**的高可用，每次用户**Session**的更新和获取都可以快速完成。大大提高效率。

#### [6.2. list](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_62-list)

1. **介绍** ：**list** 即是 **链表**。C 语言并没有实现链表，所以 Redis 实现了自己的链表数据结构。Redis 的 list 的实现为一个 **双向链表**，即可以支持**反向查找和遍历**，更方便操作，不过带来了部分额外的内存开销。
2. **常用命令:** `rpush,lpop,lpush,rpop,lrange、llen` 等。
3. **应用场景:** 发布与订阅或者说消息队列、慢查询。
   1. **消息队列：Redis**的链表结构，可以轻松实现阻塞队列，可以使用左进右出的命令组成来完成队列的设计。比如：数据的生产者可以通过**Lpush**命令从左边插入数据，多个数据消费者，可以使用**BRpop**命令阻塞的“抢”列表尾部的数据。
   2. 文章列表或者数据分页展示的应用。

rpush、lpop实现队列

rpush、rpop实现栈

![redis list](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\redis-list.png)

#### [6.3. hash](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_63-hash)

1. **介绍** ：hash 类似于 JDK1.8 前的 HashMap，内部实现也差不多(数组 + 链表)。不过，Redis 的 hash 做了更多优化。另外，hash 是一个 string 类型的 field 和 value 的映射表，**特别适合用于存储对象**，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以 hash 数据结构来存储用户信息，商品信息等等。

2. **常用命令：** `hset,hmset,hexists,hget,hgetall,hkeys,hvals` 等。

3. **应用场景:** 系统中对象数据的存储。

   [redis]() hash扩容

   在Redis中，键值对（Key-Value Pair）存储方式是由字典（Dict）保存的，而字典底层是通过哈希表来实现的。通过哈希表中的节点保存字典中的键值对。我们知道当HashMap中由于Hash冲突（负载因子）超过某个阈值时，出于链表性能的考虑，会进行Resize的操作。Redis也一样。

   使用了一种叫做渐进式哈希(rehashing)的机制来提高字典的缩放效率，避免 rehash 对服务器性能造成影响，渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均摊到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量。

   在redis中，扩展或收缩哈希表需要将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面， 但是， 这个 rehash 动作并不是一次性、集中式地完成的， 而是分多次、渐进式地完成的。为了避免 rehash 对服务器性能造成影响， 服务器不是一次性将 ht[0] 里面的所有键值对全部 rehash 到 ht[1] ， 而是分多次、渐进式地将 ht[0] 里面的键值对慢慢地 rehash 到 ht[1] 。

   **以下是哈希表渐进式 rehash 的详细步骤：**

   （1）为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表。

   （2）在字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始。

   （3）在 rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一。

   （4）随着字典操作的不断执行， 最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成。

   渐进式 rehash 的好处在于它采取**分而治之的方式**， 将 **rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量**。

#### [6.4. set](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_64-set)

1. **介绍 ：** set 类似于 Java 中的 `HashSet` 。Redis 中的 set 类型是一种**无序集合**，集合中的元素没有先后顺序。当你需要存储一个列表数据，又不希望出现重复数据时，set 是一个很好的选择，并且 **set 提供了判断某个成员是否在一个 set 集合内的重要接口**，这个也是 list 所不能提供的。可以基于 set 轻易实现**交集、并集、差集**的操作。比如：你可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis 可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程。
2. **常用命令：** `sadd,spop,smembers,sismember,scard,sinterstore,sunion` 等。
3. **应用场景:** 需要存放的数据不能重复以及需要获取多个数据源交集和并集等场景

#### [6.5. sorted set](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_65-sorted-set)

1. **介绍：** 和 set 相比，sorted set 增加了一个**权重参数 score**，使得集合中的元素能够按 score 进行有序排列，还可以通过 score 的范围来获取元素的列表。有点像是 Java 中 HashMap 和 TreeSet 的结合体。 给输入的元素加了一个权重参数，可以按权重进行指定范围升序排序
2. **常用命令：** `zadd,zcard,zscore,zrange,zrevrange,zrem` 等。
3. **应用场景：** 需要对数据根据某个权重进行排序的场景。比如在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维度的消息排行榜）等信息。微博热搜榜，就是有个后面的热度值，前面就是名称。

#### [6.6 bitmap](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_66-bitmap)

1. **介绍 ：** bitmap 存储的是连续的二进制数字（0 和 1），通过 bitmap, 只需要一个 bit 位来表示某个元素对应的值或者状态，key 就是对应元素本身 。我们知道 8 个 bit 可以组成一个 byte，所以 bitmap 本身会极大的节省储存空间。按bit位来存储信息
2. **常用命令：** `setbit` 、`getbit` 、`bitcount`、`bitop`
3. **应用场景:** 适合需要保存状态信息（比如是否签到、是否登录...）并需要进一步对这些信息进行分析的场景。比如用户签到情况、活跃用户情况、用户行为统计（比如是否点赞过某个视频）。

### [7. Redis 单线程模型详解](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_7-redis-单线程模型详解)

Redis 作为一个内存服务器，**它需要处理很多来自外部的网络请求**，它使用 I/O 多路复用机制同时监听多个文件描述符的可读和可写状态，一旦收到网络请求就会在内存中快速处理，由于绝大多数的操作都是纯内存的，所以处理的速度会非常地快。

**Redis 基于 Reactor 模式来设计开发了自己的一套高效的事件处理模型** 

这套事件处理模型对应的是 Redis 中的**文件事件处理器（file event handler）**。由于文件事件处理器（file event handler）是单线程方式运行的，所以我们一般都说 Redis 是单线程模型。

**既然是单线程，那怎么监听大量的客户端连接呢？**

redis通过**IO多路复用程序**来监听来自客户端的大量连接（socket），它将感兴趣的事件及类型（读、写）注册到内核中并监听每个事件是否发生

好处：多路复用技术的使用让redis不需要额外创建多余的线程来监听客户端的大量连接，降低了资源的消耗

此外，redis是一个**事件驱动程序**，需要处理**文件事件**和**时间事件**

> 文件事件：客户端进行读取写入等操作，涉及一系列网路通信
>
> Redis 基于 Reactor 模式开发了自己的网络事件处理器：这个处理器被称为文件事件处理器（file event handler）。文件事件处理器**使用 I/O 多路复用（multiplexing）程序**来同时监听多个套接字，并根据 套接字目前执行的任务来为套接字关联不同的事件处理器。
>
> 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关 闭（close）等操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。
>
> **虽然文件事件处理器以单线程方式运行，但通过使用 I/O 多路复用程序来监听多个套接字**，文件事件处理器既实现了高性能的网络通信模型，又可以很好地与 Redis 服务器中其他同样以单线程方式运行的模块进行对接，这保持了 Redis 内部单线程设计的简单性。

![img](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\redis事件处理器.png)

### [8. Redis 没有使用多线程？为什么不使用多线程？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_8-redis-没有使用多线程？为什么不使用多线程？)

虽然说 Redis 是单线程模型，但是， 实际上，**Redis 在 4.0 之后的版本中就已经加入了对多线程的支持。**

不过，Redis 4.0 增加的多线程主要是**针对一些大键值对的删除操作的命令**，使用这些命令就会使用主处理之外的其他线程来“异步处理”。

大体上来说，**Redis 6.0 之前主要还是单线程处理。**

**那，Redis6.0 之前 为什么不使用多线程？**

我觉得主要原因有下面 3 个：

1. 单线程编程容易并且更容易维护；
2. Redis 的性能瓶颈不再 CPU ，主要在内存和网络；
3. 多线程就会存在死锁、线程上下文切换等问题，甚至会影响性能。

### [9. Redis6.0 之后为何引入了多线程？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_9-redis60-之后为何引入了多线程？)

**Redis6.0 引入多线程主要是为了提高网络 IO 读写性能**，因为这个算是 Redis 中的一个性能瓶颈（Redis 的瓶颈主要受限于内存和网络）。

虽然，Redis6.0 引入了多线程，但是 Redis 的多线程只是在网络数据的读写这类耗时操作上使用了， **执行命令仍然是单线程顺序执行**。因此，你也不需要担心线程安全问题。

Redis6.0 的多线程默认是禁用的，只使用主线程。如需开启需要修改 redis 配置文件 `redis.conf` ：

```bash
io-threads-do-reads yesCopy to clipboardErrorCopied
```

开启多线程后，还需要设置线程数，否则是不生效的。同样需要修改 redis 配置文件 `redis.conf` :

```bash
io-threads 4 #官网建议4核的机器建议设置为2或3个线程，8核的建议设置为6个线程
```

### [10. Redis 给缓存数据设置过期时间有啥用？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_10-redis-给缓存数据设置过期时间有啥用？)

- **因为内存是有限的，如果缓存中的所有数据都是一直保存的话，分分钟直接 Out of memory。**

> 注意：**Redis 中除了字符串类型有自己独有设置过期时间的命令 `setex` 外，其他方法都需要依靠 `expire` 命令来设置过期时间 。另外， `persist` 命令可以移除一个键的过期时间： **

- 很多时候，我们的**业务场景就是需要某个数据只在某一时间段内存在**，比如我们的短信验证码可能只在 1 分钟内有效，用户登录的 token 可能只在 1 天内有效。

  如果**使用传统的数据库来处理的话，一般都是自己判断过期，这样更麻烦并且性能要差很多**。

  需要自己写一个函数用来判断是否在有效期内

### [11. Redis 是如何判断数据是否过期的呢？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_11-redis-是如何判断数据是否过期的呢？)

Redis 通过一个叫做**过期字典**（可以看作是 hash 表）来保存数据过期的时间。过期字典的键指向 Redis 数据库中的某个 key(键)，过期字典的值是一个 long long 类型的整数，这个整数保存了 key 所指向的数据库键的过期时间（毫秒精度的 UNIX 时间戳）。

![redis过期字典](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\redis过期时间.png)

### [12. 过期的数据的删除策略了解么？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_12-过期的数据的删除策略了解么？)

常用的过期数据的删除策略就两个（重要！）：

1. **惰性删除** ：只会在**取出 key 的时候才对数据进行过期检查**。这样对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。
2. **定期删除** ： **每隔一段时间抽取一批 key 执行删除过期 key 操作**。并且，Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。

定期删除对内存更加友好，惰性删除对 CPU 更加友好。两者各有千秋，所以 Redis 采用的是 **定期删除+惰性/懒汉式删除** 。

但是，仅仅通过给 key 设置过期时间还是有问题的。因为还是可能存在定期删除和惰性删除漏掉了很多过期 key 的情况。这样就导致大量过期 key 堆积在内存里，然后就 Out of memory 了。

怎么解决这个问题呢？答案就是： **Redis 内存淘汰机制。**

### [13. Redis 内存淘汰机制了解么？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_13-redis-内存淘汰机制了解么？)

> 相关问题：MySQL 里有 2000w 数据，Redis 中只存 20w 的数据，如何保证 Redis 中的数据都是热点数据?

Redis 提供 6 种数据淘汰策略：

1. **volatile-lru（least recently used）**：从**已设置过期时间的数据集**（server.db[i].expires）中挑选最近最少使用的数据淘汰
2. **volatile-ttl**：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰
3. **volatile-random**：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰
4. **allkeys-lru（least recently used）**：当内存不足以容纳新写入数据时，在**键空间中**，移除最近最少使用的 key（这个是最常用的）
5. **allkeys-random**：从数据集（server.db[i].dict）中任意选择数据淘汰
6. **no-eviction**：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！

4.0 版本后增加以下两种：

7. **volatile-lfu（least frequently used）**：从已设置过期时间的数据集(server.db[i].expires)中挑选**最不经常使用的数据淘汰**
8. **allkeys-lfu（least frequently used）**：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key

### [14. Redis 持久化机制(怎么保证 Redis 挂掉之后再重启数据可以进行恢复)](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_14-redis-持久化机制怎么保证-redis-挂掉之后再重启数据可以进行恢复)

持久化数据将内存中的数据写入到硬盘中，大部分原因是为了之后重用数据（重启及其、及其故障之后恢复数据），为了防止系统故障而将数据备份到一个远程位置。

redis不同步memcached很重要的一点是redis支持持久化，而且支持两种不同的持久化操作，redis的一种持久化方式叫快照（snapshotting，RDB），另外一种叫只追加文件（append-only file，AOF）

**快照（snapshotting）持久化（RDB）**

redis通过**创建快照来获得存储在内存中的数据在某个时间点上的副本**。在创建快照之后，可以对快照备份，将快照复制到其他服务器从而创建具有相同数据的服务器副本，还可以将快照留在原地以便重启服务器的时候使用

快照持久化是redis默认的持久化方式，bgsave

**AOF（append-only file）持久化**

与快照持久化相比，AOF 持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下 Redis 没有开启 AOF（append only file）方式的持久化，可以通过 appendonly 参数开启。

开启 AOF 持久化后**每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入硬盘中的 AOF 文件**。AOF 文件的保存位置和 RDB 文件的位置相同，都是通过 dir 参数设置的，默认的文件名是 appendonly.aof。

redis的配置文件中有三种不同的AOF持久化方式

```conf
appendfsync always    #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度
appendfsync everysec  #每秒钟同步一次，显示地将多个写命令同步到硬盘
appendfsync no        #让操作系统决定何时进行同步
```

为了兼顾数据和写入性能，可以选用每秒同步的方式，这对redis的性能几乎没有影响，即便出现系统崩溃，也只会丢失一秒的数据。如果硬盘忙于执行写入操作的时候，redis会放慢自己的速度以便适应硬盘的最大写入速度。

**Redis 4.0 对于持久化机制的优化**

Redis 4.0 开始支持 RDB 和 AOF 的**混合持久化**（默认关闭，可以通过配置项 `aof-use-rdb-preamble` 开启）。

如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, **快速加载同时避免丢失过多的数据**。当然缺点也是有的， **AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差**。

**AOF 重写**

AOF 重写可以产生一个新的 AOF 文件，这个新的 AOF 文件和原有的 AOF 文件所保存的数据库状态一样，但体积更小。

AOF 重写是一个有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无须对现有 AOF 文件进行任何读入、分析或者写入操作。

在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会**在子进程创建新 AOF 文件期间，记录服务器执行的所有写命令**。**当子进程完成创建新 AOF 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新 AOF 文件的末尾，使得新旧两个 AOF 文件所保存的数据库状态一致**。最后，服务器用新的 AOF 文件替换旧的 AOF 文件，以此来完成 AOF 文件重写操作

### [15. Redis 事务](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_15-redis-事务)

Redis 可以通过 **MULTI，EXEC，DISCARD 和 WATCH** 等命令来实现事务(transaction)功能。

```bash
> MULTI
OK
> INCR foo
QUEUED
> INCR bar
QUEUED
> EXEC
1) (integer) 1
2) (integer) 1Copy to clipboardErrorCopied
```

使用 [MULTI](https://redis.io/commands/multi)命令后可以输入多个命令。Redis 不会立即执行这些命令，而是将它们放到队列，当调用了[EXEC](https://redis.io/commands/exec)命令将执行所有命令。

但是，Redis 的事务和我们平时理解的关系型数据库的事务不同：

**Redis 是不支持 roll back 的，也就是是一个命令执行失败，其后的命令还是会被执行，因而不满足原子性的（而且不满足持久性）。**

但满足隔离性和一致性

>Redis 官网也解释了自己为啥不支持回滚。简单来说就是 Redis 开发者们觉得没必要支持回滚，这样更**简单便捷并且性能更好**。Redis 开发者觉得**即使命令执行错误也应该在开发过程中就被发现而不是生产过程中**。

**对Redis事务的理解：redis事务提供了一种将多个命令请求打包的功能，然后再按顺序执行打包的所有命令，并且不会被中途打断。**

### [16. 缓存穿透](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_16-缓存穿透)

#### [16.1. 什么是缓存穿透？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_161-什么是缓存穿透？)

缓存穿透说简单点就是**大量请求的 key 根本不存在于缓存中，导致请求直接到了数据库上，根本没有经过缓存这一层**。举个例子：某个黑客故意制造我们缓存中不存在的 key 发起大量请求，导致大量请求落到数据库。会导致数据库的压力很大，很消耗内存，每次磁盘I/O都需要消耗资源

#### [16.2. 缓存穿透情况的处理流程是怎样的？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_162-缓存穿透情况的处理流程是怎样的？)

**用户的请求最终都要跑到数据库中查询一遍。**

用户发起大量请求（key不存在于缓存中）——>查询缓存中是否存在对应的数据——>不存在——>查询数据库中是否存在对应的数据——>不存在——>返回空数据

#### [16.3. 有哪些解决办法？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_163-有哪些解决办法？)

最基本的就是**首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端**。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。

**1）缓存无效 key**

**如果缓存和数据库都查不到某个 key 的数据就写一个到 Redis 中去并设置过期时间**，具体命令如下： `SET key value EX 10086` 。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求 key，会导致 Redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。这样可以应对大量无效的key，直接返回null，又解决了黑客更改key的麻烦，我定期缓存时间只有60s，超时就过期。

**2）布隆过滤器**

用于判断一个给定的数据是否存在于海量数据中，需要判断key是否合法。

具体实现：将所有可能存在的请求的值都存放在布隆过滤器中，当用户的请求过来，先判断用户发来的请求致是否存在于布隆过滤器中，不存在的化，直接返回请求参数错误信息给客户端，存在的化才会走以下流程。

![image](C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\加入布隆过滤器后的缓存处理流程.png)

需要注意的是，布隆过滤器会存在误判的情况，当布隆过滤器说某个元素key不存在时，那么这个元素一定不存在，说某个元素key存在时，那么不一定存在，小概率误判。

**为什么布隆过滤器会出现误判的情况呢? （布隆过滤器的原理）**

![布隆过滤器hash计算](https://camo.githubusercontent.com/bff9320e4e3bb9f59b072e1c40419c1f3c5122b3e12c4781733014f00318fad6/68747470733a2f2f6d792d626c6f672d746f2d7573652e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323031392d31312f2545352542382538332545392539412538362545382542462538372545362542422541342545352539392541382d686173682545382542462539302545372541452539372e706e67)

- 当一个元素加入布隆过滤器中时：

  - 使用布隆过滤器中的hash函数对元素值进行计算，得到hash值（有几个函数就有几个hash值）
  - 根据得到得hash值，在位数组中将对应下标的值置为1

- 判断一个元素是否存在于布隆过滤器的时：

  - 对给定元素进行hash计算
  - 得到值之后判断位数组中的每个hash值对应的下标是否为1，如果值都为1，那么说明这个值在布隆过滤器中，如果存在一个值不为1，说明该元素不存在布隆过滤器中

  **！**不同的字符串可能hash出来的位置相同，**hash冲突**

### 17 缓存雪崩

#### [17.1. 什么是缓存雪崩？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_171-什么是缓存雪崩？)

- **缓存在同一时间大面积失效，后面的请求直接落到了数据库上，造成数据库短时间内承受大量请求，好比雪崩一样，数据库可能就此宕机**。

  举个例子：系统的缓存模块出问题比如宕机导致不可用。造成系统的所有访问都要走数据库。

- **有一些被大量访问数据（热点缓存）在某一时刻大面积失效，导致对应的请求直接落到了数据上。**

  举个例子 ：秒杀开始 12 个小时之前，我们统一存放了一批商品到 Redis 中，设置的缓存过期时间也是 12 个小时，**那么秒杀开始的时候，这些秒杀的商品的访问直接就失效了（过期）**。导致的情况就是，相应的请求直接就落到了数据库上，就像雪崩一样可怕。

#### [17.2. 有哪些解决办法？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_172-有哪些解决办法？)

**针对 Redis 服务不可用的情况：**

1. 采用 **Redis 集群**，避免单机出现问题整个缓存服务都没办法使用。
2. **限流**，避免同时处理大量的请求。

**针对热点缓存失效的情况：**

1. **设置不同的失效时间**比如**随机设置缓存的失效时间**。
2. **缓存永不失效**。

### [18. 如何保证缓存和数据库数据的一致性？](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/redis-all?id=_18-如何保证缓存和数据库数据的一致性？)

我个人觉得引入缓存之后，如果为了短时间的不一致性问题，选择让系统设计变得更加复杂的话，完全没必要。

下面单独对 **Cache Aside Pattern（旁路缓存模式）** 来聊聊。

Cache Aside Pattern 中遇到写请求是这样的：**更新 DB，然后直接删除 cache** 。

如果更新数据库成功，而删除缓存这一步失败（让人以为还有，但库存已经没了，导致超卖）的情况的话，简单说两个解决方案：

1. **缓存失效时间变短（不推荐，治标不治本）** ：我们让缓存数据的过期时间变短，这样的话缓存就会从数据库中加载数据。另外，这种解决办法对于先操作缓存后操作数据库的场景不适用。
2. **增加 cache 更新重试机制（常用）**： 如果 cache 服务当前不可用导致缓存删除失败的话，我们就隔一段时间进行重试，重试次数可以自己定。如果多次重试还是失败的话，我们可以把当前更新失败的 key 存入队列中，等缓存服务可用之后，再将 缓存中对应的 key 删除即可

### 19.三种缓存读写策略

#### [Cache Aside Pattern（旁路缓存模式）](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/3种常用的缓存读写策略?id=cache-aside-pattern（旁路缓存模式）)

旁路缓存模式是平时使用比较多的一个缓存读写模式，比较适合**读请求比较多**的场景。

服务端需要同时维系DB和cache，并且是**以DB的结果为准**。

**写：**

- 先更新DB
- 然后直接删除cache

<img src="C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\5687fe759a1dac9ed9554d27e3a23b6d.png" alt="img" style="zoom:80%;" />

**读：**

- 从cache中读取数据，读取到就直接返回
- cache读不到，就从DB读取数据返回
- 再把数据放到cache中

<img src="C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\a8c18b5f5b1aed03234bcbbd8c173a87.png" alt="img" style="zoom:80%;" />

 ***在写的过程中，可以先删除cache，后更新DB吗？***

- 不可以，这会造成数据库和缓存数据不一致，比如请求1先写数据A，请求2随后读数据A的话就可能产生数据不一致的问题。

  这个过程可以描述为：

  - 请求1把cache中的数据A删除，cache中是没有数据A了的
  - 请求2只能从DB中读数据，并把数据写入cache
  - 请求1把数据A在DB中更新
  - **这时DB是请求1写入的数据，而cache是原来从DB中读出来的数据**

***为什么在写数据的过程中，先更新DB，后删除cache就没有问题了？***

- 理论上还是会出现数据不一致，但是概率很小，因为**缓存的写入速度比数据库的写入速度快很多**。比如**请求1先读数据A，请求2随后写数据A，并且数据A不在缓存中的话**也可能产生数据不一致的问题。

  这个过程可以描述为：

  - 请求1从DB读数据A
  - 请求2写更新数据A到数据库并把cache中的数据A删除
  - 请求1将数据A写入cache
  - **这时DB的数据是请求2写的数据，而cache中的数据是原来从DB中读出来的数据**

**Cache Aside Pattern 的缺陷**

**缺陷1：首次请求数据一定不在 cache 的问题**

- 可以将热点数据提前加入cache中

**缺陷2：写操作比较频繁的话导致cache中的数据会被频繁被删除，这样会影响缓存命中率 。**

（所以适合读操作比较频繁的状况）

- 数据库和缓存数据羟乙酯的场景：更新DB的时候同样需要更新cache，不过我们需要加一个锁/分布式锁来保证更新cache的时候不存在线程安全问题
- 可以短暂允许数据库和缓存数据不一致的场景：更新DB的时候同时更新cache，但是给缓存加一个比较短的过期时间，这样就可以保证即使数据不一致的话影响也比较小

#### [Read/Write Through Pattern（读写穿透）](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/3种常用的缓存读写策略?id=readwrite-through-pattern（读写穿透）)

Read/Write Through Pattern 中服务端把 cache 视为主要数据存储，从中读取数据并将数据写入其中。cache 服务负责将此数据读取和写入 DB，从而减轻了应用程序的职责。

> 比较少见，抛去性能方面的影响，大概率是因为我们经常使用的分布式缓存 Redis 并没有提供 cache 将数据写入DB的功能。

**写（Write Through）：**

- 先查 cache，cache 中不存在，直接更新 DB。
- cache 中存在，则先更新 cache，然后 cache 服务自己更新 DB（**同步更新 cache 和 DB**）。

简单画了一张图帮助大家理解写的步骤。

<img src="C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\20210201100340808.png" alt="img" style="zoom:80%;" />

**读(Read Through)：**

- 从 cache 中读取数据，读取到就直接返回 。
- 读取不到的话，先从 DB 加载，写入到 cache 后返回响应。

简单画了一张图帮助大家理解读的步骤。

<img src="C:\Users\94307\OneDrive - zju.edu.cn\learnbm\JAVA\学习笔记\9ada757c78614934aca11306f334638d.png" alt="img" style="zoom:80%;" />

Read-Through Pattern 实际只是在 Cache-Aside Pattern 之上进行了封装。在 Cache-Aside Pattern 下，发生读请求的时候，如果 cache 中不存在对应的数据，是由客户端自己负责把数据写入 cache，而 Read Through Pattern 则是 cache 服务自己来写入缓存的，这对客户端是透明的。

和 Cache Aside Pattern 一样， Read-Through Pattern 也有**首次请求数据一定不在 cache 的问题，对于热点数据可以提前放入缓存中**。

#### [Write Behind Pattern（异步缓存写入）](https://snailclimb.gitee.io/javaguide/#/docs/database/Redis/3种常用的缓存读写策略?id=write-behind-pattern（异步缓存写入）)

Write Behind Pattern 和 Read/Write Through Pattern 很相似，两者都是由 cache 服务来负责 cache 和 DB 的读写。

Write Behind Pattern 和 Read/Write Through Pattern 很相似，两者都是由 cache 服务来负责 cache 和 DB 的读写。

但是，两个又有很大的不同：**Read/Write Through 是同步更新 cache 和 DB，而 Write Behind Caching 则是只更新缓存，不直接更新 DB，而是改为异步批量的方式来更新 DB。**

很明显，这种方式对**数据一致性**带来了更大的挑战，比如cache数据可能还没异步更新DB的话，cache服务可能就就挂掉了。

**Write Behind Pattern 下 DB 的写性能非常高，非常适合一些数据经常变化又对数据一致性要求没那么高的场景，比如浏览量、点赞量。**

https://mp.weixin.qq.com/s/tB1cZqiX6pc7T1ITTnbr7A
